{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "We want to train an Atari game, fine-tuning a Multi-game DT"
      ],
      "metadata": {
        "id": "WWHOa9VbjfKT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For some trainings, maybe you will need a colab with \"high capacity RAM\" (colab Pro)"
      ],
      "metadata": {
        "id": "ziiv8xkWPWnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/takuseno/d4rl-atari\n",
        "\n",
        "# ACHTUNG You will get an error but don't worry. Press the button 'restart session' and run this cell again"
      ],
      "metadata": {
        "id": "4fhg-3TX6CQY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO maybe it's not necessary\n",
        "!pip install gym[atari]\n",
        "!pip install autorom[accept-rom-license]"
      ],
      "metadata": {
        "id": "OLzbZo0z6IY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch import Tensor\n",
        "\n",
        "from typing import Mapping, Optional, Tuple\n",
        "import math\n",
        "import numpy as np\n",
        "import scipy\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "OPv4bFlo2-7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CONVERT action numbers in a game into all_game numbers\n",
        "\n",
        "# MsPacman' actions\n",
        "GAME_ACTIONS = {\n",
        "  \"NOOP\": 0,\n",
        "  \"UP\": 1,\n",
        "  \"RIGHT\": 2,\n",
        "  \"LEFT\": 3,\n",
        "  \"DOWN\": 4,\n",
        "  \"UPRIGHT\": 5,\n",
        "  \"UPLEFT\": 6,\n",
        "  \"DOWNRIGHT\": 7,\n",
        "  \"DOWNLEFT\": 8,\n",
        "}\n",
        "\n",
        "ALL_GAME_ACTION = {\n",
        "  \"NOOP\": 0,\n",
        "  \"FIRE\": 1,\n",
        "  \"UP\": 2,\n",
        "  \"RIGHT\": 3,\n",
        "  \"LEFT\": 4,\n",
        "  \"DOWN\": 5,\n",
        "  \"UPRIGHT\": 6,\n",
        "  \"UPLEFT\": 7,\n",
        "  \"DOWNRIGHT\": 8,\n",
        "  \"DOWNLEFT\": 9,\n",
        "  \"UPFIRE\": 10,\n",
        "  \"RIGHTFIRE\": 11,\n",
        "  \"LEFTFIRE\": 12,\n",
        "  \"DOWNFIRE\": 13,\n",
        "  \"UPRIGHTFIRE\": 14,\n",
        "  \"UPLEFTFIRE\": 15,\n",
        "  \"DOWNRIGHTFIRE\": 16,\n",
        "  \"DOWNLEFTFIRE\": 17,\n",
        "}\n",
        "\n",
        "def action_to_allgameaction(action):\n",
        "  all_action = list(GAME_ACTIONS.keys())[list(GAME_ACTIONS.values()).index(action)]\n",
        "  return ALL_GAME_ACTION[all_action]\n",
        "\n",
        "def allgameaction_to_action(all_action):\n",
        "  action = list(ALL_GAME_ACTION.keys())[list(ALL_GAME_ACTION.values()).index(all_action)]\n",
        "  return GAME_ACTIONS[action]\n"
      ],
      "metadata": {
        "id": "K2IfWu-O0QYA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Breakout\n",
        "#act_voc = 4\n",
        "#context_length = 30\n",
        "#batch_size = 128\n",
        "#game_name = 'breakout-mixed-v4'\n",
        "#target_reward = 90\n",
        "\n",
        "# Pong\n",
        "#act_voc = 6\n",
        "#context_length = 50\n",
        "#batch_size = 512\n",
        "#game_name = pong-expert-v4'\n",
        "#target_reward = 20\n",
        "\n",
        "# Packman\n",
        "act_voc = 18 # 18=all_game_actions 9=ms-pacman\n",
        "context_length = 4 # default multi-game-DT\n",
        "batch_size = 1 # 256 in paper, but we don't have enough GPU ram\n",
        "game_name = 'ms-pacman-expert-v4'\n",
        "target_reward = 1150 #180?\n",
        "\n",
        "# Qbert\n",
        "#act_voc = 6\n",
        "#context_length = 30\n",
        "#batch_size = 128\n",
        "#game_name = 'qbert-expert-v4'\n",
        "#target_reward = 14000\n",
        "\n",
        "hparams = {\n",
        "    'game_name': game_name,\n",
        "    'n_layer':6, # num of blocks?\n",
        "    'n_head': 8, #\n",
        "    'n_embd':128,\n",
        "    'context_length':context_length, # 30 in breakout, 50 in pong\n",
        "    'dropout':0.1, # dropout value\n",
        "    'act_voc':act_voc, # in breakout 4, 6 in Pong\n",
        "    'batch_size': batch_size,\n",
        "    'state_dim': 1*84*84, # no skacked images\n",
        "    'max_timestep': 4096, # hardcoded default 4096\n",
        "    'lr': 1e-4, # TODO adjust the lr for fine-tuning\n",
        "    'wt_decay':1e-4,\n",
        "    'warmup_steps':10000,\n",
        "    'target_reward': target_reward, # 90=Breakout, Pong=20\n",
        "    'max_epochs':5\n",
        "}"
      ],
      "metadata": {
        "id": "3C8ay4GGDWI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MULTI-GAME DECISION TRANSFORMER\n"
      ],
      "metadata": {
        "id": "kHthx6_zjoaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/rwightman/pytorch-image-models/blob/29fda20e6d428bf636090ab207bbcf60617570ca/timm/layers/weight_init.py#L99\n",
        "def variance_scaling_(tensor: Tensor, scale=1.0, mode=\"fan_in\", distribution=\"trunc_normal\") -> Tensor:\n",
        "    fan_in, fan_out = nn.init._calculate_fan_in_and_fan_out(tensor)\n",
        "    if mode == \"fan_in\":\n",
        "        scale /= max(1.0, fan_in)\n",
        "    elif mode == \"fan_out\":\n",
        "        scale /= max(1.0, fan_out)\n",
        "    elif mode == \"fan_avg\":\n",
        "        scale /= max(1.0, (fan_in + fan_out) / 2.0)\n",
        "\n",
        "    if distribution == \"trunc_normal\":\n",
        "        stddev = np.sqrt(scale)\n",
        "        # Adjust stddev for truncation.\n",
        "        # Constant from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\n",
        "        stddev = stddev / 0.87962566103423978\n",
        "        return nn.init.trunc_normal_(tensor, std=stddev)\n",
        "    elif distribution == \"normal\":\n",
        "        stddev = np.sqrt(scale)\n",
        "        return nn.init.normal_(tensor, std=stddev)\n",
        "    elif distribution == \"uniform\":\n",
        "        limit = np.sqrt(3.0 * scale)\n",
        "        return nn.init.uniform_(tensor, -limit, limit)\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid distribution: {distribution}\")\n",
        "\n",
        "\n",
        "def sample_from_logits(\n",
        "    logits: Tensor,\n",
        "    generator: Optional[torch.Generator] = None,\n",
        "    deterministic: Optional[bool] = False,\n",
        "    temperature: Optional[float] = 1e0,\n",
        "    top_k: Optional[int] = None,\n",
        "    top_percentile: Optional[float] = None,\n",
        ") -> Tuple[Tensor, Tensor]:\n",
        "    r\"\"\"Generate a categorical sample from given logits.\"\"\"\n",
        "    if deterministic:\n",
        "        sample = torch.argmax(logits, dim=-1)\n",
        "    else:\n",
        "        if top_percentile is not None:\n",
        "            # percentile: 0 to 100, quantile: 0 to 1\n",
        "            percentile = torch.quantile(logits, top_percentile / 100, dim=-1)\n",
        "            logits = torch.where(logits > percentile[..., None], logits, -np.inf)\n",
        "        if top_k is not None:\n",
        "            logits, top_indices = torch.topk(logits, top_k)\n",
        "        sample = D.Categorical(logits=temperature * logits).sample()\n",
        "        # probs = F.softmax(temperature * logits, dim=-1)\n",
        "        # sample = torch.multinomial(probs, num_samples=1, generator=generator)\n",
        "        if top_k is not None:\n",
        "            sample_shape = sample.shape\n",
        "            # Flatten top-k indices and samples for easy indexing.\n",
        "            top_indices = torch.reshape(top_indices, [-1, top_k])\n",
        "            sample = sample.flatten()\n",
        "            sample = top_indices[torch.arange(len(sample)), sample]\n",
        "            # Reshape samples back to original dimensions.\n",
        "            sample = torch.reshape(sample, sample_shape)\n",
        "    return sample\n",
        "\n",
        "\n",
        "def encode_reward(rew: Tensor) -> Tensor:\n",
        "    r\"\"\"Encode reward values into values expected by the model.\"\"\"\n",
        "    # 0: no reward   1: positive reward   2: terminal reward   3: negative reward\n",
        "    rew = (rew > 0) * 1 + (rew < 0) * 3\n",
        "    return rew.to(dtype=torch.int32)\n",
        "\n",
        "\n",
        "def encode_return(ret: Tensor, ret_range: Tuple[int]) -> Tensor:\n",
        "    r\"\"\"Encode (possibly negative) return values into discrete return tokens.\"\"\"\n",
        "    ret = ret.to(dtype=torch.int32)\n",
        "    ret = torch.clip(ret, ret_range[0], ret_range[1]-1)\n",
        "    ret = ret - ret_range[0]\n",
        "    return ret\n",
        "\n",
        "def cross_entropy(logits, labels):\n",
        "    r\"\"\"Applies sparse cross entropy loss between logits and target labels.\"\"\"\n",
        "    labels = F.one_hot(labels.long(), logits.shape[-1]).to(dtype=logits.dtype)\n",
        "    loss = -labels * F.log_softmax(logits, dim=-1)\n",
        "    return torch.mean(loss)\n",
        "\n",
        "\n",
        "def decode_return(ret: torch.Tensor, ret_range: Tuple[int]) -> torch.Tensor:\n",
        "    ret = ret.to(dtype=torch.int32)\n",
        "    ret = ret + ret_range[0]\n",
        "    return ret"
      ],
      "metadata": {
        "id": "rafNq6Bj2Npp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DT code from https://github.com/nikhilbarhate99/min-decision-transformer\n",
        "class MLP(nn.Module):\n",
        "    r\"\"\"A 2-layer MLP which widens then narrows the input.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_dim: int,\n",
        "        init_scale: float,\n",
        "        widening_factor: int = 4,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self._init_scale = init_scale\n",
        "        self._widening_factor = widening_factor\n",
        "\n",
        "        self.fc1 = nn.Linear(in_dim, self._widening_factor * in_dim)\n",
        "        self.act = nn.GELU(approximate=\"tanh\")\n",
        "        self.fc2 = nn.Linear(self._widening_factor * in_dim, in_dim)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        variance_scaling_(self.fc1.weight, scale=self._init_scale)\n",
        "        nn.init.zeros_(self.fc1.bias)\n",
        "        variance_scaling_(self.fc2.weight, scale=self._init_scale)\n",
        "        nn.init.zeros_(self.fc2.bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        w_init_scale: Optional[float] = None,\n",
        "        qkv_bias: bool = True,\n",
        "        proj_bias: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, \"dim should be divisible by num_heads\"\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim**-0.5\n",
        "        self.w_init_scale = w_init_scale\n",
        "\n",
        "        self.qkv = nn.Linear(dim, 3 * dim, bias=qkv_bias)\n",
        "        self.proj = nn.Linear(dim, dim, bias=proj_bias)\n",
        "\n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        variance_scaling_(self.qkv.weight, scale=self.w_init_scale)\n",
        "        if self.qkv.bias is not None:\n",
        "            nn.init.zeros_(self.qkv.bias)\n",
        "        variance_scaling_(self.proj.weight, scale=self.w_init_scale)\n",
        "        if self.proj.bias is not None:\n",
        "            nn.init.zeros_(self.proj.bias)\n",
        "\n",
        "    def forward(self, x, mask: Optional[Tensor] = None) -> Tensor:\n",
        "        B, T, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, T, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv.unbind(0)  # make torchscript happy (cannot use tensor as tuple)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        if mask is not None:\n",
        "            mask_value = -torch.finfo(attn.dtype).max  # max_neg_value\n",
        "            attn = attn.masked_fill(~mask.to(dtype=torch.bool), mask_value)\n",
        "\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, T, C)\n",
        "        x = self.proj(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class CausalSelfAttention(Attention):\n",
        "    r\"\"\"Self attention with a causal mask applied.\"\"\"\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x: Tensor,\n",
        "        mask: Optional[Tensor] = None,\n",
        "        custom_causal_mask: Optional[Tensor] = None,\n",
        "        prefix_length: Optional[int] = 0,\n",
        "    ) -> Tensor:\n",
        "        if x.ndim != 3:\n",
        "            raise ValueError(\"Expect queries of shape [B, T, D].\")\n",
        "\n",
        "        seq_len = x.shape[1]\n",
        "        # If custom_causal_mask is None, the default causality assumption is\n",
        "        # sequential (a lower triangular causal mask).\n",
        "        causal_mask = custom_causal_mask\n",
        "        if causal_mask is None:\n",
        "            device = x.device\n",
        "            causal_mask = torch.tril(torch.ones((seq_len, seq_len), dtype=torch.bool, device=device))\n",
        "        causal_mask = causal_mask[None, None, :, :]\n",
        "\n",
        "        # Similar to T5, tokens up to prefix_length can all attend to each other.\n",
        "        causal_mask[:, :, :, :prefix_length] = 1\n",
        "        mask = mask * causal_mask if mask is not None else causal_mask\n",
        "\n",
        "        return super().forward(x, mask)\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, embed_dim: int, num_heads: int, init_scale: float, dropout_rate: float):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(embed_dim)\n",
        "        self.attn = CausalSelfAttention(embed_dim, num_heads=num_heads, w_init_scale=init_scale)\n",
        "        self.dropout_1 = nn.Dropout(dropout_rate)\n",
        "\n",
        "        self.ln_2 = nn.LayerNorm(embed_dim)\n",
        "        self.mlp = MLP(embed_dim, init_scale)\n",
        "        self.dropout_2 = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x, **kwargs):\n",
        "        x = x + self.dropout_1(self.attn(self.ln_1(x), **kwargs))\n",
        "        x = x + self.dropout_2(self.mlp(self.ln_2(x)))\n",
        "        return x\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    r\"\"\"A transformer stack.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        num_layers: int,\n",
        "        dropout_rate: float,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self._num_layers = num_layers\n",
        "        self._num_heads = num_heads\n",
        "        self._dropout_rate = dropout_rate\n",
        "\n",
        "        init_scale = 2.0 / self._num_layers\n",
        "        self.layers = nn.ModuleList([])\n",
        "        for _ in range(self._num_layers):\n",
        "            block = Block(embed_dim, num_heads, init_scale, dropout_rate)\n",
        "            self.layers.append(block)\n",
        "        self.norm_f = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        h: Tensor,\n",
        "        mask: Optional[Tensor] = None,\n",
        "        custom_causal_mask: Optional[Tensor] = None,\n",
        "        prefix_length: Optional[int] = 0,\n",
        "    ) -> Tensor:\n",
        "        r\"\"\"Connects the transformer.\n",
        "\n",
        "        Args:\n",
        "        h: Inputs, [B, T, D].\n",
        "        mask: Padding mask, [B, T].\n",
        "        custom_causal_mask: Customized causal mask, [T, T].\n",
        "        prefix_length: Number of prefix tokens that can all attend to each other.\n",
        "\n",
        "        Returns:\n",
        "        Array of shape [B, T, D].\n",
        "        \"\"\"\n",
        "        if mask is not None:\n",
        "            # Make sure we're not passing any information about masked h.\n",
        "            h = h * mask[:, :, None]\n",
        "            mask = mask[:, None, None, :]\n",
        "\n",
        "        for block in self.layers:\n",
        "            h = block(\n",
        "                h,\n",
        "                mask=mask,\n",
        "                custom_causal_mask=custom_causal_mask,\n",
        "                prefix_length=prefix_length,\n",
        "            )\n",
        "        h = self.norm_f(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class MultiGameDecisionTransformer(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        img_size: Tuple[int],\n",
        "        patch_size: Tuple[int],\n",
        "        num_actions: int,\n",
        "        num_rewards: int,\n",
        "        return_range: Tuple[int],\n",
        "        d_model: int,\n",
        "        num_layers: int,\n",
        "        dropout_rate: float,\n",
        "        predict_reward: bool,\n",
        "        single_return_token: bool,\n",
        "        conv_dim: int,\n",
        "        num_steps: int,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # Expected by the transformer model.\n",
        "        if d_model % 64 != 0:\n",
        "            raise ValueError(f\"Model size {d_model} must be divisible by 64\")\n",
        "\n",
        "        self.img_size = img_size   # (84,84)\n",
        "        self.patch_size = patch_size # (16,16)\n",
        "        self.num_actions = num_actions # 18\n",
        "        self.num_rewards = num_rewards # 4\n",
        "        self.num_returns = return_range[1] - return_range[0]  # [-20, 100] -> 120\n",
        "        #print('self.num_returns ', self.num_returns)\n",
        "        self.return_range = return_range\n",
        "        self.d_model = d_model # 1280 embedd_dim\n",
        "        self.predict_reward = predict_reward\n",
        "        self.conv_dim = conv_dim\n",
        "        self.single_return_token = single_return_token\n",
        "        self.spatial_tokens = True\n",
        "        self.num_steps = num_steps\n",
        "\n",
        "        self.transformer = Transformer(\n",
        "            embed_dim=self.d_model,\n",
        "            num_heads=self.d_model // 64,\n",
        "            num_layers=num_layers,\n",
        "            dropout_rate=dropout_rate,\n",
        "        )\n",
        "\n",
        "        patch_height, patch_width = self.patch_size[0], self.patch_size[1]\n",
        "        # If img_size=(84, 84), patch_size=(14, 14), then P = 84 / 14 = 6.\n",
        "        self.image_emb = nn.Conv2d(\n",
        "            in_channels=1,\n",
        "            out_channels=self.d_model, # ???? 1280 channels out??\n",
        "            kernel_size=(patch_height, patch_width),\n",
        "            stride=(patch_height, patch_width),\n",
        "            padding=\"valid\",\n",
        "        )  # image_emb is now [BT x D x P x P].\n",
        "        patch_grid = (self.img_size[0] // self.patch_size[0], self.img_size[1] // self.patch_size[1]) # (84/14,84/14)->(6,6)\n",
        "        num_patches = patch_grid[0] * patch_grid[1]\n",
        "        self.image_pos_enc = nn.Parameter(torch.randn(1, 1, num_patches, self.d_model)) # ??? pos_enc randn?\n",
        "\n",
        "        self.ret_emb = nn.Embedding(self.num_returns+1, self.d_model) # 121 -> embedd_dim\n",
        "        self.act_emb = nn.Embedding(self.num_actions, self.d_model) # 18 -> embedd_dim\n",
        "        if self.predict_reward:\n",
        "            self.rew_emb = nn.Embedding(self.num_rewards, self.d_model) # 4 -> embedd_dim\n",
        "\n",
        "        #num_steps = 4 # ?????\n",
        "        num_obs_tokens = num_patches if self.spatial_tokens else 1 # 36 = 6*6\n",
        "        if self.predict_reward:\n",
        "            tokens_per_step = num_obs_tokens + 3    # 39\n",
        "        else:\n",
        "            tokens_per_step = num_obs_tokens + 2\n",
        "        self.positional_embedding = nn.Parameter(torch.randn(tokens_per_step * self.num_steps, self.d_model)) # (159, 1280)\n",
        "\n",
        "        # prediction heads\n",
        "        self.ret_linear = nn.Linear(self.d_model, self.num_returns+1)\n",
        "        self.act_linear = nn.Linear(self.d_model, self.num_actions)\n",
        "        if self.predict_reward:\n",
        "            self.rew_linear = nn.Linear(self.d_model, self.num_rewards)\n",
        "\n",
        "    def sequence_loss(self, inputs: Mapping[str, Tensor], model_outputs: Mapping[str, Tensor]) -> Tensor:\n",
        "        r\"\"\"Compute the loss on data wrt model outputs.\"\"\"\n",
        "        obj_pairs = self._objective_pairs(inputs, model_outputs)\n",
        "        obj = [cross_entropy(logits, target) for logits, target in obj_pairs]\n",
        "        return sum(obj) / len(obj)\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        nn.init.trunc_normal_(self.image_emb.weight, std=0.02)\n",
        "        nn.init.zeros_(self.image_emb.bias)\n",
        "        nn.init.normal_(self.image_pos_enc, std=0.02)\n",
        "\n",
        "        nn.init.trunc_normal_(self.ret_emb.weight, std=0.02)\n",
        "        nn.init.trunc_normal_(self.act_emb.weight, std=0.02)\n",
        "        if self.predict_reward:\n",
        "            nn.init.trunc_normal_(self.rew_emb.weight, std=0.02)\n",
        "\n",
        "        nn.init.trunc_normal_(self.positional_embedding, std=0.02)\n",
        "\n",
        "        variance_scaling_(self.ret_linear.weight)\n",
        "        nn.init.zeros_(self.ret_linear.bias)\n",
        "        variance_scaling_(self.act_linear.weight)\n",
        "        nn.init.zeros_(self.act_linear.bias)\n",
        "        if self.predict_reward:\n",
        "            variance_scaling_(self.rew_linear.weight)\n",
        "            nn.init.zeros_(self.rew_linear.bias)\n",
        "\n",
        "    def _image_embedding(self, image: Tensor):\n",
        "        r\"\"\"Embed [B x T x C x W x H] images to tokens [B x T x output_dim] tokens.\n",
        "\n",
        "        Args:\n",
        "            image: [B x T x C x W x H] image to embed.\n",
        "\n",
        "        Returns:\n",
        "            Image embedding of shape [B x T x output_dim] or [B x T x _ x output_dim].\n",
        "        \"\"\"\n",
        "        assert len(image.shape) == 5 # ha de tenir 5 dimensions\n",
        "        image_dims = image.shape[-3:] # -> [C x W x H]\n",
        "        batch_dims = image.shape[:2] # -> [B x T]\n",
        "\n",
        "        # Reshape to [BT x C x H x W].\n",
        "        image = torch.reshape(image, (-1,) + image_dims)\n",
        "        # Perform any-image specific processing.\n",
        "        image = image.to(dtype=torch.float32) / 255.0\n",
        "\n",
        "        # split in patches\n",
        "        image_emb = self.image_emb(image)  # [BT x D x P x P]\n",
        "        # haiku.Conv2D is channel-last, so permute before reshape below for consistency\n",
        "        image_emb = image_emb.permute(0, 2, 3, 1)  # [BT x P x P x D]\n",
        "        # Reshape to [B x T x P*P x D].\n",
        "        image_emb = torch.reshape(image_emb, batch_dims + (-1, self.d_model))\n",
        "        image_emb = image_emb + self.image_pos_enc\n",
        "        return image_emb\n",
        "\n",
        "    def _embed_inputs(self, obs: Tensor, ret: Tensor, act: Tensor, rew: Tensor) -> Tuple[Tensor, Tensor, Tensor, Tensor]:\n",
        "        # Embed only prefix_frames first observations.\n",
        "        # obs are [B x T x C x H x W].\n",
        "        #print('obs ', obs.shape)\n",
        "        obs_emb = self._image_embedding(obs) # output -> Image embedding of shape [B x T x output_dim] or [B x T x _ x output_dim].\n",
        "        #print('obs_emb ', obs_emb.shape)\n",
        "        # Embed returns and actions\n",
        "        # Encode returns.\n",
        "        #print('ret ', ret)\n",
        "        ret = encode_return(ret, self.return_range)\n",
        "        rew = encode_reward(rew)\n",
        "        #print('ret ', ret)\n",
        "        #print('max_ret ', torch.max(ret))\n",
        "        act_emb = self.act_emb(act)\n",
        "        ret_emb = self.ret_emb(ret)\n",
        "        if self.predict_reward:\n",
        "            rew_emb = self.rew_emb(rew)\n",
        "        else:\n",
        "            rew_emb = None\n",
        "        return obs_emb, ret_emb, act_emb, rew_emb\n",
        "\n",
        "    def forward(self, inputs: Mapping[str, Tensor]) -> Mapping[str, Tensor]:\n",
        "        r\"\"\"Process sequence.\"\"\"\n",
        "        num_batch = inputs[\"actions\"].shape[0]\n",
        "        num_steps = inputs[\"actions\"].shape[1]\n",
        "        # Embed inputs.\n",
        "        obs_emb, ret_emb, act_emb, rew_emb = self._embed_inputs(\n",
        "            inputs[\"observations\"],\n",
        "            inputs[\"returns-to-go\"],\n",
        "            inputs[\"actions\"],\n",
        "            inputs[\"rewards\"],\n",
        "        )\n",
        "        device = obs_emb.device\n",
        "\n",
        "        #print('obs_emb0 ', obs_emb.shape)\n",
        "        if self.spatial_tokens:\n",
        "            # obs is [B x T x W x D]\n",
        "            num_obs_tokens = obs_emb.shape[2]\n",
        "            obs_emb = torch.reshape(obs_emb, obs_emb.shape[:2] + (-1,))\n",
        "            # obs is [B x T x W*D]\n",
        "        else:\n",
        "            num_obs_tokens = 1\n",
        "        # Collect sequence.\n",
        "        # Embeddings are [B x T x D].\n",
        "\n",
        "        #print('obs_emb ', obs_emb.shape)\n",
        "        #print('ret_emb ', ret_emb.shape)\n",
        "        #print('act_emb ', act_emb.shape)\n",
        "        #print('rew_emb ', rew_emb.shape)\n",
        "\n",
        "        if self.predict_reward:\n",
        "            token_emb = torch.cat([obs_emb, ret_emb, act_emb, rew_emb], dim=-1)\n",
        "            tokens_per_step = num_obs_tokens + 3\n",
        "            # sequence is [obs ret act rew ... obs ret act rew]\n",
        "        else:\n",
        "            token_emb = torch.cat([obs_emb, ret_emb, act_emb], dim=-1)\n",
        "            tokens_per_step = num_obs_tokens + 2\n",
        "            # sequence is [obs ret act ... obs ret act]\n",
        "        token_emb = torch.reshape(token_emb, [num_batch, tokens_per_step * num_steps, self.d_model])\n",
        "        # Create position embeddings.\n",
        "\n",
        "        token_emb = token_emb + self.positional_embedding\n",
        "        # Run the transformer over the inputs.\n",
        "\n",
        "        # Token dropout.\n",
        "        batch_size = token_emb.shape[0]\n",
        "        obs_mask = np.ones([batch_size, num_steps, num_obs_tokens], dtype=bool)\n",
        "        ret_mask = np.ones([batch_size, num_steps, 1], dtype=bool)\n",
        "        act_mask = np.ones([batch_size, num_steps, 1], dtype=bool)\n",
        "        rew_mask = np.ones([batch_size, num_steps, 1], dtype=bool)\n",
        "\n",
        "        # Mask out all return tokens expect the first one.\n",
        "        ret_mask[:, 1:] = 0\n",
        "\n",
        "        if self.predict_reward:\n",
        "            mask = [obs_mask, ret_mask, act_mask, rew_mask]\n",
        "        else:\n",
        "            mask = [obs_mask, ret_mask, act_mask]\n",
        "        mask = np.concatenate(mask, axis=-1)\n",
        "        mask = np.reshape(mask, [batch_size, tokens_per_step * num_steps])\n",
        "        mask = torch.tensor(mask, dtype=torch.bool, device=device)\n",
        "\n",
        "        custom_causal_mask = None\n",
        "        if self.spatial_tokens:\n",
        "            # Temporal transformer by default assumes sequential causal relation.\n",
        "            # This makes the transformer causal mask a lower triangular matrix.\n",
        "            #     P1 P2 R  a  P1 P2 ... (Ps: image patches)\n",
        "            # P1  1  0* 0  0  0  0\n",
        "            # P2  1  1  0  0  0  0\n",
        "            # R   1  1  1  0  0  0\n",
        "            # a   1  1  1  1  0  0\n",
        "            # P1  1  1  1  1  1  0*\n",
        "            # P2  1  1  1  1  1  1\n",
        "            # ... (0*s should be replaced with 1s in the ideal case)\n",
        "            # But, when we have multiple tokens for an image (e.g. patch tokens, conv\n",
        "            # feature map tokens, etc) as inputs to transformer, this assumption does\n",
        "            # not hold, because there is no sequential dependencies between tokens.\n",
        "            # Therefore, the ideal causal mask should not mask out tokens that belong\n",
        "            # to the same images from each others.\n",
        "            seq_len = token_emb.shape[1]\n",
        "            sequential_causal_mask = np.tril(np.ones((seq_len, seq_len)))\n",
        "            num_timesteps = seq_len // tokens_per_step\n",
        "            num_non_obs_tokens = tokens_per_step - num_obs_tokens\n",
        "            diag = [\n",
        "                np.ones((num_obs_tokens, num_obs_tokens)) if i % 2 == 0 else np.zeros((num_non_obs_tokens, num_non_obs_tokens))\n",
        "                for i in range(num_timesteps * 2)\n",
        "            ]\n",
        "            block_diag = scipy.linalg.block_diag(*diag)\n",
        "            custom_causal_mask = np.logical_or(sequential_causal_mask, block_diag)\n",
        "            custom_causal_mask = torch.tensor(custom_causal_mask, dtype=torch.bool, device=device)\n",
        "\n",
        "        output_emb = self.transformer(token_emb, mask, custom_causal_mask)\n",
        "\n",
        "        # Output_embeddings are [B x 3T x D].\n",
        "        # Next token predictions (tokens one before their actual place).\n",
        "        ret_pred = output_emb[:, (num_obs_tokens - 1) :: tokens_per_step, :]\n",
        "        act_pred = output_emb[:, (num_obs_tokens - 0) :: tokens_per_step, :]\n",
        "        embeds = torch.cat([ret_pred, act_pred], dim=-1)\n",
        "        # Project to appropriate dimensionality.\n",
        "        ret_pred = self.ret_linear(ret_pred)\n",
        "        act_pred = self.act_linear(act_pred)\n",
        "        # Return logits as well as pre-logits embedding.\n",
        "        result_dict = {\n",
        "            \"embeds\": embeds,\n",
        "            \"action_logits\": act_pred,\n",
        "            \"return_logits\": ret_pred,\n",
        "        }\n",
        "        if self.predict_reward:\n",
        "            rew_pred = output_emb[:, (num_obs_tokens + 1) :: tokens_per_step, :]\n",
        "            rew_pred = self.rew_linear(rew_pred)\n",
        "            result_dict[\"reward_logits\"] = rew_pred\n",
        "        # Return evaluation metrics.\n",
        "        result_dict[\"loss\"] = self.sequence_loss(inputs, result_dict)\n",
        "        #result_dict[\"accuracy\"] = self.sequence_accuracy(inputs, result_dict)\n",
        "        return result_dict\n",
        "\n",
        "    def _objective_pairs(self, inputs: Mapping[str, Tensor], model_outputs: Mapping[str, Tensor]) -> Tensor:\n",
        "        r\"\"\"Get logit-target pairs for the model objective terms.\"\"\"\n",
        "        act_target = inputs[\"actions\"]\n",
        "        ret_target = encode_return(inputs[\"returns-to-go\"], self.return_range)\n",
        "        act_logits = model_outputs[\"action_logits\"]\n",
        "        ret_logits = model_outputs[\"return_logits\"]\n",
        "\n",
        "        #single token return\n",
        "        ret_target = ret_target[:, :1]\n",
        "        ret_logits = ret_logits[:, :1, :]\n",
        "\n",
        "        obj_pairs = [(act_logits, act_target), (ret_logits, ret_target)]\n",
        "        if self.predict_reward:\n",
        "            rew_target = encode_reward(inputs[\"rewards\"])\n",
        "            rew_logits = model_outputs[\"reward_logits\"]\n",
        "            obj_pairs.append((rew_logits, rew_target))\n",
        "        return obj_pairs\n",
        "\n",
        "\n",
        "\n",
        "    def optimal_action(\n",
        "        self,\n",
        "        inputs: Mapping[str, Tensor],\n",
        "        return_range: Tuple[int] = (-100, 100),\n",
        "        single_return_token: bool = True,\n",
        "        opt_weight: Optional[float] = 0.0,\n",
        "        num_samples: Optional[int] = 128,\n",
        "        action_temperature: Optional[float] = 1.0,\n",
        "        return_temperature: Optional[float] = 1.0,\n",
        "        action_top_percentile: Optional[float] = None,\n",
        "        return_top_percentile: Optional[float] = None,\n",
        "        rng: Optional[torch.Generator] = None,\n",
        "        deterministic: bool = False,\n",
        "    ):\n",
        "        r\"\"\"Calculate optimal action for the given sequence model.\"\"\"\n",
        "        logits_fn = self.forward\n",
        "        obs, act, rew = inputs[\"observations\"], inputs[\"actions\"], inputs[\"rewards\"]\n",
        "        assert len(obs.shape) == 5\n",
        "        assert len(act.shape) == 2\n",
        "        inputs = {\n",
        "            \"observations\": obs,\n",
        "            \"actions\": act,\n",
        "            \"rewards\": rew,\n",
        "            \"returns-to-go\": torch.zeros_like(act),\n",
        "        }\n",
        "        sequence_length = obs.shape[1]\n",
        "        # Use samples from the last timestep.\n",
        "        timestep = -1\n",
        "        # A biased sampling function that prefers sampling larger returns.\n",
        "        def ret_sample_fn(rng, logits):\n",
        "            assert len(logits.shape) == 2\n",
        "            # Add optimality bias.\n",
        "            if opt_weight > 0.0:\n",
        "                # Calculate log of P(optimality=1|return) := exp(return) / Z.\n",
        "                logits_opt = torch.linspace(0.0, 1.0, logits.shape[1])\n",
        "                logits_opt = torch.repeat_interleave(logits_opt[None, :], logits.shape[0], dim=0)\n",
        "                # Sample from log[P(optimality=1|return)*P(return)].\n",
        "                logits = logits + opt_weight * logits_opt\n",
        "            logits = torch.repeat_interleave(logits[None, ...], num_samples, dim=0)\n",
        "            ret_sample = sample_from_logits(\n",
        "                logits,\n",
        "                generator=rng,\n",
        "                deterministic=deterministic,\n",
        "                temperature=return_temperature,\n",
        "                top_percentile=return_top_percentile,\n",
        "            )\n",
        "            # Pick the highest return sample.\n",
        "            ret_sample, _ = torch.max(ret_sample, dim=0)\n",
        "            # Convert return tokens into return values.\n",
        "            ret_sample = decode_return(ret_sample, return_range)\n",
        "            return ret_sample\n",
        "\n",
        "        # Set returns-to-go with an (optimistic) autoregressive sample.\n",
        "        # Since only first return is used by the model, only sample that (faster).\n",
        "        ret_logits = logits_fn(inputs)[\"return_logits\"][:, 0, :]\n",
        "        ret_sample = ret_sample_fn(rng, ret_logits)\n",
        "        inputs[\"returns-to-go\"][:, 0] = ret_sample\n",
        "\n",
        "        # Generate a sample from action logits.\n",
        "        act_logits = logits_fn(inputs)[\"action_logits\"][:, timestep, :]\n",
        "        act_sample = sample_from_logits(\n",
        "            act_logits,\n",
        "            generator=rng,\n",
        "            deterministic=deterministic,\n",
        "            temperature=action_temperature,\n",
        "            top_percentile=action_top_percentile,\n",
        "        )\n",
        "        return act_sample\n"
      ],
      "metadata": {
        "id": "wnkuA5htMrCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MODEL TESTER\n",
        "\n",
        "# CREATE THE MODEL\n",
        "# maxTimestep = max steps in a trajectory\n",
        "'''model_gpt = DecisionTransformer(\n",
        "                state_dim=hparams['state_dim'],\n",
        "                act_voc=hparams['act_voc'],\n",
        "                n_blocks=hparams['n_layer'],\n",
        "                h_dim=hparams['n_embd'],\n",
        "                context_len=3,\n",
        "                n_heads=hparams['n_head'],\n",
        "                drop_p=hparams['dropout']\n",
        "            )\n",
        "model_gpt.to(device)'''\n",
        "\n",
        "OBSERVATION_SHAPE = (84, 84)\n",
        "PATCH_SHAPE = (14, 14)\n",
        "NUM_ACTIONS = 18  # Maximum number of actions in the full dataset.\n",
        "# rew=0: no reward, rew=1: score a point, rew=2: end game rew=3: lose a point\n",
        "NUM_REWARDS = 4\n",
        "RETURN_RANGE = [-20, 100]  # A reasonable range of returns identified in the dataset, quantized in 120 discrete values\n",
        "\n",
        "model = MultiGameDecisionTransformer(\n",
        "    img_size=OBSERVATION_SHAPE,\n",
        "    patch_size=PATCH_SHAPE,\n",
        "    num_actions=NUM_ACTIONS,\n",
        "    num_rewards=NUM_REWARDS,\n",
        "    return_range=RETURN_RANGE,\n",
        "    d_model=1280,\n",
        "    num_layers=10,\n",
        "    dropout_rate=0.1,\n",
        "    predict_reward=True,\n",
        "    single_return_token=True,\n",
        "    conv_dim=256,\n",
        "    num_steps=4, # fixed sequence len in training\n",
        ").to(device)\n"
      ],
      "metadata": {
        "id": "-oI4Bhi5pu9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "inputs = {}\n",
        "#states_d  torch.Size([256, 4, 1, 84, 84])\n",
        "#actions_d  torch.Size([256, 4])\n",
        "#rtgs_d  torch.Size([256, 4])\n",
        "#rewards_d  torch.Size([256, 4])\n",
        "\n",
        "inputs[\"observations\"] = torch.zeros((1,4, 1, 84, 84), dtype=torch.float32).to(device)\n",
        "inputs[\"actions\"] = torch.zeros((1,4), dtype=torch.long).to(device)\n",
        "inputs[\"returns-to-go\"] = 130*torch.ones((1,4), dtype=torch.float32).to(device)\n",
        "inputs[\"rewards\"] = torch.zeros((1,4), dtype=torch.float32).to(device)\n",
        "#timesteps=torch.zeros((1, 3), dtype=torch.int64).to(device)\n",
        "model.to(device)\n",
        "model.forward(inputs)\n"
      ],
      "metadata": {
        "id": "Ev0p1CE2PaMu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD MULTI-GAMR WEIGHTS"
      ],
      "metadata": {
        "id": "H1_KO5aX8_cs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_jax_weights(model, model_params):\n",
        "    def load_ln(m, k):\n",
        "        m.weight.data = torch.from_numpy(model_params[k][\"scale\"])\n",
        "        m.bias.data = torch.from_numpy(model_params[k][\"offset\"])\n",
        "\n",
        "    def load_linear(m, k):\n",
        "        m.weight.data = torch.from_numpy(model_params[k][\"w\"]).t()\n",
        "        m.bias.data = torch.from_numpy(model_params[k][\"b\"])\n",
        "\n",
        "    def load_attn(attn, k):\n",
        "        qkv_w = np.concatenate(\n",
        "            [\n",
        "                model_params[k + \"/query\"][\"w\"],\n",
        "                model_params[k + \"/key\"][\"w\"],\n",
        "                model_params[k + \"/value\"][\"w\"],\n",
        "            ],\n",
        "            axis=-1,\n",
        "        )\n",
        "        attn.qkv.weight.data = torch.from_numpy(qkv_w).t()\n",
        "\n",
        "        qkv_b = np.concatenate(\n",
        "            [\n",
        "                model_params[k + \"/query\"][\"b\"],\n",
        "                model_params[k + \"/key\"][\"b\"],\n",
        "                model_params[k + \"/value\"][\"b\"],\n",
        "            ]\n",
        "        )\n",
        "        attn.qkv.bias.data = torch.from_numpy(qkv_b)\n",
        "\n",
        "        load_linear(attn.proj, k + \"/linear\")\n",
        "\n",
        "    def load_mlp(mlp, k):\n",
        "        load_linear(mlp.fc1, k + \"/linear\")\n",
        "        load_linear(mlp.fc2, k + \"/linear_1\")\n",
        "\n",
        "    def load_transformer(transformer):\n",
        "        prefix = \"decision_transformer/~/sequence\"\n",
        "        for i in range(transformer._num_layers):\n",
        "            block = transformer.layers[i]\n",
        "\n",
        "            load_ln(block.ln_1, f\"{prefix}/h{i}_ln_1\")\n",
        "            load_attn(block.attn, f\"{prefix}/h{i}_attn\")\n",
        "\n",
        "            load_ln(block.ln_2, f\"{prefix}/h{i}_ln_2\")\n",
        "            load_mlp(block.mlp, f\"{prefix}/h{i}_mlp\")\n",
        "        load_ln(transformer.norm_f, f\"{prefix}/ln_f\")\n",
        "\n",
        "    def load_embedding(m, k):\n",
        "        m.weight.data = torch.from_numpy(model_params[k][\"embeddings\"])\n",
        "\n",
        "    def load_image_emb(m, k):\n",
        "        # [H x W x Cin x Cout] -> [Cout, Cin, H, W]\n",
        "        m.weight.data = torch.from_numpy(model_params[k][\"w\"]).permute(3, 2, 0, 1)\n",
        "        m.bias.data = torch.from_numpy(model_params[k][\"b\"])\n",
        "\n",
        "    # --- Load transformer\n",
        "\n",
        "    load_transformer(model.transformer)\n",
        "\n",
        "    # --- Load model\n",
        "\n",
        "    load_linear(model.act_linear, \"decision_transformer/act_linear\")\n",
        "    load_linear(model.ret_linear, \"decision_transformer/ret_linear\")\n",
        "    if model.predict_reward:\n",
        "        load_linear(model.rew_linear, \"decision_transformer/rew_linear\")\n",
        "\n",
        "    model.image_pos_enc = nn.Parameter(torch.tensor(model_params[\"decision_transformer\"][\"image_pos_enc\"]))\n",
        "    model.positional_embedding = nn.Parameter(torch.tensor(model_params[\"decision_transformer\"][\"positional_embeddings\"]))\n",
        "\n",
        "    load_image_emb(model.image_emb, \"decision_transformer/~_embed_inputs/image_emb\")\n",
        "\n",
        "    load_embedding(model.ret_emb, \"decision_transformer/~_embed_inputs/embed\")\n",
        "    load_embedding(model.act_emb, \"decision_transformer/~_embed_inputs/embed_1\")\n",
        "    if model.predict_reward:\n",
        "        load_embedding(model.rew_emb, \"decision_transformer/~_embed_inputs/embed_2\")\n"
      ],
      "metadata": {
        "id": "Nl70Jsds9BqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get Multi-game checkpoints from owr gdrive\n",
        "!pip install --upgrade gdown\n",
        "!gdown \"https://drive.google.com/uc?id=1tFB_hmhztUtm9SQdIEMgipkI6SJLqJP3\"\n",
        "!gdown \"https://drive.google.com/uc?id=1EXx38g6p5J-XH_zP0Km1mCjKxCZbQ6M_\""
      ],
      "metadata": {
        "id": "O57djszq9MFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load multi-game checkpoints\n",
        "model_params = torch.load(\"/content/drive/MyDrive/TMP/model_params.pt\")\n",
        "model_state = torch.load(\"/content/drive/MyDrive/TMP/model_state.pt\")\n",
        "\n",
        "load_jax_weights(model, model_params)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "23UOZFRb9H9x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SET THE ENVIRONMENT"
      ],
      "metadata": {
        "id": "pTP7OlDMtHBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import d4rl_atari\n",
        "\n",
        "env = gym.make(hparams['game_name'])\n",
        "\n",
        "print(env.action_space)\n",
        "\n",
        "env.reset() # (1, 84, 84)\n",
        "\n"
      ],
      "metadata": {
        "id": "wgmere8H6ZNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SETUP THE DATASET of our game"
      ],
      "metadata": {
        "id": "8Hs-TmZOEEr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataset():\n",
        "\n",
        "      # GET THE DATA !!\n",
        "      dataset = env.get_dataset()\n",
        "\n",
        "      obs_data = dataset['observations'] # observation data in (1000000, 1, 84, 84)\n",
        "      action_data = dataset['actions'] # action data in (1000000,)\n",
        "      reward_data = dataset['rewards'] # reward data in (1000000,)\n",
        "      terminal_data = dataset['terminals'] # terminal flags in (1000000,)\n",
        "\n",
        "      plt.imshow(obs_data[1000][0])\n",
        "      plt.show()\n",
        "\n",
        "      terminal_pos = np.where(terminal_data==1)[0]\n",
        "      terminal_data = None # de-allocate mem\n",
        "      print(\"num episodes \", terminal_pos.shape)\n",
        "\n",
        "      # -- create reward-to-go dataset\n",
        "      start_index = 0\n",
        "      rtg = np.zeros_like(reward_data)\n",
        "      for i in terminal_pos:\n",
        "          i = int(i)\n",
        "          curr_traj_returns = reward_data[start_index:i]\n",
        "          reward_acum = 0\n",
        "          for j in range(i-1, start_index-1, -1): # start from i-1\n",
        "              reward_acum += reward_data[j]\n",
        "              #rtg_j = curr_traj_returns[j-start_index:i-start_index]\n",
        "              rtg[j] = reward_acum\n",
        "          start_index = i\n",
        "      print('max rtg is %d' % max(rtg))\n",
        "\n",
        "      #reward_data = None\n",
        "\n",
        "      # -- create timestep dataset ******************************\n",
        "      start_index = 0\n",
        "      timesteps = np.zeros(len(action_data), dtype=int)\n",
        "      for i in terminal_pos:\n",
        "          timesteps[start_index:i] = np.arange(i - start_index)\n",
        "          start_index = i\n",
        "\n",
        "      # convert game_ation numbers into all_game_action numbers\n",
        "      print(len(action_data))\n",
        "      print(action_data[0])\n",
        "      for i in range(len(action_data)):\n",
        "        action_data[i] = action_to_allgameaction(action_data[i])\n",
        "\n",
        "      max_timestep = max(timesteps)\n",
        "      print('max timestep is %d' % max_timestep)\n",
        "      print(\"***** data loaded **********\")\n",
        "\n",
        "\n",
        "      return obs_data, action_data, terminal_pos, rtg, timesteps, max_timestep, reward_data\n",
        "\n"
      ],
      "metadata": {
        "id": "FbJHIqoNn_YQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Class that picks up a block of data from the dataset\n",
        "class StateActionReturnDataset(Dataset):\n",
        "\n",
        "    def __init__(self, obs, actions, done_idxs, rtgs, timesteps, rewards):\n",
        "        self.context_length = hparams['context_length']\n",
        "        self.obs = obs\n",
        "        self.actions = actions\n",
        "        self.rewards = rewards\n",
        "        self.done_idxs = done_idxs\n",
        "        self.rtgs = rtgs\n",
        "        self.timesteps = timesteps\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.obs) - self.context_length * 3\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # to avoid blocks in between of 2 trajectories, if the idx is too close to the end of a trajectory, re-position\n",
        "        # the idx to a context_length away to the end of the trajectory\n",
        "        done_idx = idx + self.context_length\n",
        "        for i in self.done_idxs:\n",
        "            if i > idx: # first done_idx greater than idx\n",
        "                done_idx = min(int(i), done_idx)\n",
        "                break\n",
        "        idx = done_idx - self.context_length\n",
        "        if idx < 0:\n",
        "            idx = 0\n",
        "            done_idx = idx + self.context_length\n",
        "        states = torch.tensor(np.array(self.obs[idx:done_idx]), dtype=torch.float32) #.reshape(self.context_length, -1) # (self.context_length, 4*84*84)\n",
        "        states = states / 255. # normalize obs\n",
        "        actions = torch.tensor(self.actions[idx:done_idx], dtype=torch.long)#.unsqueeze(1) # (self.context_length, 1)\n",
        "        rtgs = torch.tensor(self.rtgs[idx:done_idx], dtype=torch.float32)#.unsqueeze(1)\n",
        "        rewards = torch.tensor(self.rewards[idx:done_idx], dtype=torch.float32)#.unsqueeze(1)\n",
        "        #timesteps = torch.tensor(self.timesteps[idx:done_idx], dtype=torch.int64).unsqueeze(1) # (1,1)\n",
        "\n",
        "        return states, actions, rtgs, rewards"
      ],
      "metadata": {
        "id": "vU89QS67vjWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE THE DATASET\n",
        "obss, actions, done_idxs, rtgs, timesteps, maxTimestep, rewards = create_dataset()\n",
        "#hparams['max_timestep'] = maxTimestep\n",
        "\n",
        "# CREATE A CLASS FOR THE DATALOADER TO GET DATA\n",
        "train_dataset = StateActionReturnDataset(obss, actions, done_idxs, rtgs, timesteps, rewards)\n"
      ],
      "metadata": {
        "id": "h7kufEszv2dW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "TRAINING"
      ],
      "metadata": {
        "id": "H0vfA0ytJrlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_action(\n",
        "    inputs,\n",
        "    model,\n",
        "    return_range,\n",
        "    single_return_token,\n",
        "    opt_weight: Optional[float] = 0.0,\n",
        "    num_samples: Optional[int] = 128,\n",
        "    action_temperature: Optional[float] = 1.0,\n",
        "    return_temperature: Optional[float] = 1.0,\n",
        "    action_top_percentile: Optional[float] = None,\n",
        "    return_top_percentile: Optional[float] = None,\n",
        "):\n",
        "    obs, act, rew = inputs['observations'], inputs['actions'], inputs['rewards']\n",
        "    assert len(obs.shape) == 5\n",
        "    assert len(act.shape) == 2\n",
        "    act = act[:, -1].unsqueeze(1)\n",
        "    inputs['actions'] = act\n",
        "    inputs['rewards'] = rew[:, -1].unsqueeze(1)\n",
        "    inputs['returns-to-go'] = torch.zeros_like(act)\n",
        "    seq_len = obs.shape[1]\n",
        "    timesteps = -1\n",
        "\n",
        "    def ret_sample_fn(logits):\n",
        "        assert len(logits.shape) == 2\n",
        "        # Add optimality bias\n",
        "        if opt_weight > 0.0:\n",
        "            # Calculate log of P(optimality|return) = exp(return)/Z\n",
        "            logits_opt = torch.linspace(0., 1., logits.shape[1])\n",
        "            logits_opt = torch.repeat_interleave(\n",
        "                logits_opt.unsqueeze(0), logits.shape[0], dim=0)\n",
        "            # Sample from log[P(optimality=1|return)*P(return)]\n",
        "            logits = logits + opt_weight * logits_opt\n",
        "        logits = torch.repeat_interleave(\n",
        "            logits.unsqueeze(0), num_samples, dim=0)\n",
        "        ret_sample = sample_from_logits(\n",
        "            logits, temperature=return_temperature, top_percentile=return_top_percentile)\n",
        "        # pick the highest return sample\n",
        "        ret_sample = torch.max(ret_sample)\n",
        "        # ret_sample = torch.max(ret_sample, dim=0)\n",
        "        # Convert return tokens into return values\n",
        "        ret_sample = decode_return(ret_sample, return_range)\n",
        "        return ret_sample\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if single_return_token:\n",
        "            ret_logits = model(inputs)['return_logits'][:, 0, :]\n",
        "            ret_sample = ret_sample_fn(ret_logits)\n",
        "            inputs['returns-to-go'][:, 0] = ret_sample\n",
        "        else:\n",
        "            # Auto-regressively regenerate all return tokens in a sequence\n",
        "            def ret_logits_fn(ipts): return model(ipts)['return_logits']\n",
        "            ret_sample = autoregressive_generate(inputs, ret_logits_fn, 'returns-to-go', seq_len, ret_sample_fn)\n",
        "            inputs['returns-to-go'] = ret_sample\n",
        "\n",
        "        # Generate a sample from action logits\n",
        "        act_logits = model(inputs)['action_logits'][:, timesteps, :]\n",
        "        act_sample = sample_from_logits(\n",
        "            act_logits, temperature=action_temperature,\n",
        "            top_percentile=action_top_percentile)\n",
        "    return act_sample\n"
      ],
      "metadata": {
        "id": "qlHbvmPCn4z8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "def trainer(model, dataloader):\n",
        "\n",
        "        #optimizer = torch.optim.AdamW(model.parameters(), lr=tparams['learning_rate'], betas=tparams['betas'])\n",
        "        optimizer = torch.optim.AdamW(\n",
        "                        model.parameters(),\n",
        "                        lr=hparams['lr'],\n",
        "                        weight_decay=hparams['wt_decay']\n",
        "                    )\n",
        "        scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
        "                        optimizer,\n",
        "                        lambda steps: min((steps+1)/hparams['warmup_steps'], 1)\n",
        "                        )\n",
        "\n",
        "\n",
        "        #self.tokens = 0 # counter used for learning rate decay\n",
        "\n",
        "        counter = 0\n",
        "\n",
        "        losses = []\n",
        "        losses_1000 = []\n",
        "        for epoch in range(hparams['max_epochs']):\n",
        "\n",
        "            model.to(device)\n",
        "            model.train()\n",
        "\n",
        "            loader = DataLoader(dataloader, shuffle=True, pin_memory=True, # pin_memory ???????\n",
        "                                batch_size=hparams['batch_size'])\n",
        "\n",
        "            pbar = tqdm(enumerate(loader), total=len(loader))\n",
        "            for it, (states, actions, rtgs, rewards) in pbar:\n",
        "\n",
        "                # place data on the correct device\n",
        "                states = states.to(device) # size([B, seq_len, state_dim]) state_dim = 28224\n",
        "                #print('states_d ', states.shape)\n",
        "                actions = actions.to(device) # size([B, seq_len, 1])\n",
        "                #print('acrtions_d ', actions.shape)\n",
        "                rtgs = rtgs.to(device) # size([B, seq_len, 1])\n",
        "                #print('rtgs_d ', rtgs.shape)\n",
        "                rewards = rewards.to(device) #   size([B, seq_len])\n",
        "                #print('rewards_d ', rewards.shape)\n",
        "                #action_target = torch.clone(actions).detach().to(device)\n",
        "\n",
        "                inputs = {'observations': states,\n",
        "                          'returns-to-go': rtgs,\n",
        "                          'actions': actions,\n",
        "                          'rewards': rewards}\n",
        "\n",
        "                # forward the model\n",
        "                with torch.set_grad_enabled(True):\n",
        "                  result_dict= model.forward(inputs=inputs)\n",
        "                  loss = result_dict['loss']\n",
        "                  # only consider non padded elements\n",
        "                  #action_preds = action_preds.view(-1, 1) # act_dim=1 #[traj_mask.view(-1,) > 0]\n",
        "                  #action_target = action_target.type(torch.long).squeeze(-1).view(-1, 1) #act_dim=1 #[traj_mask.view(-1,) > 0]\n",
        "                                    #loss = F.cross_entropy(action_preds.reshape(-1, action_preds.size(-1)), action_target.reshape(-1).long())\n",
        "                  losses.append(loss.detach().cpu().item())\n",
        "\n",
        "                  # backprop and update the parameters\n",
        "                  optimizer.zero_grad()\n",
        "                  loss.backward()\n",
        "                  torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
        "                  optimizer.step()\n",
        "                  scheduler.step()\n",
        "\n",
        "                    # report progress\n",
        "                pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. lr {hparams['lr']:e}\")\n",
        "\n",
        "                losses_1000.append(loss.detach().cpu().item())\n",
        "                # loss info -> tensorboard\n",
        "                if it % 1000 == 999:    # every 1000 mini-batches...\n",
        "                  counter += 1\n",
        "                  writer.add_scalar('training loss',float(np.mean(losses_1000)), counter)\n",
        "                  losses_1000 = []\n",
        "\n",
        "                    # evaluate action accuracy\n",
        "            #evaluate_on_env(model, hparams['target_reward'])\n",
        "\n",
        "        return losses\n"
      ],
      "metadata": {
        "id": "qvI0gU09PSYL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# INSTANTIATE THE MODEL\n",
        "OBSERVATION_SHAPE = (84, 84)\n",
        "PATCH_SHAPE = (14, 14)\n",
        "NUM_ACTIONS = 18  # Maximum number of actions in the full dataset.\n",
        "# rew=0: no reward, rew=1: score a point, rew=2: end game rew=3: lose a point\n",
        "NUM_REWARDS = 4\n",
        "RETURN_RANGE = [-20, 100]  # A reasonable range of returns identified in the dataset, quantized in 120 discrete values\n",
        "\n",
        "model = MultiGameDecisionTransformer(\n",
        "    img_size=OBSERVATION_SHAPE,\n",
        "    patch_size=PATCH_SHAPE,\n",
        "    num_actions=NUM_ACTIONS,\n",
        "    num_rewards=NUM_REWARDS,\n",
        "    return_range=RETURN_RANGE,\n",
        "    d_model=1280,\n",
        "    num_layers=10,\n",
        "    dropout_rate=0.1,\n",
        "    predict_reward=True,\n",
        "    single_return_token=True,\n",
        "    conv_dim=256,\n",
        "    num_steps=4, # fixed sequence len in training\n",
        ").to(device)"
      ],
      "metadata": {
        "id": "HlaubXRTal81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TRAIN THE MODEL\n",
        "losses = trainer(model, train_dataset)\n"
      ],
      "metadata": {
        "id": "WgYhJAvXFDqC"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}