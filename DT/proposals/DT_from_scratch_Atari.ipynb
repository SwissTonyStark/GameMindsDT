{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## DECISION TRANSFORMER\n",
        "A new class that inherits from  GPT2 model but modifying the input and output to work with sequences of actions, states and returns-to-go.\n",
        "\n",
        "inspired in\n",
        "https://github.com/kzl/decision-transformer\n",
        "\n",
        "A modification of the Hugging Face transformer.\n",
        "look at: gym/decision_transformer/decision_transformer.py\n"
      ],
      "metadata": {
        "id": "kHthx6_zjoaZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt"
      ],
      "metadata": {
        "id": "OPv4bFlo2-7G"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "HOViYCWJv16d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer architecture\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/transformer.png\" width=\"1000px\" alt=\"Zoom in to the Transformer\"/>\n",
        "</p>"
      ],
      "metadata": {
        "id": "wekMUPvt4Xdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ATTENTION"
      ],
      "metadata": {
        "id": "i_rPVTib4zNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# default GPT2 values?\n",
        "hparams = {\n",
        "    'n_layer':12, # num of blocks?\n",
        "    'n_head': 16, #\n",
        "    'n_embd':128,\n",
        "    'context_length':30,\n",
        "    'block_size':90, # context_length * 3\n",
        "    'dropout':0.1, # dropout value\n",
        "    'bias': True, # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster. warning! nn.LayerNorm doesn't support bias=False\n",
        "    'vocab_size':6, # in Pong\n",
        "    'batch_size': 2,\n",
        "    'path':'/content/drive/MyDrive/Deep/UPC/Projecte/datasets/Pong/expert',\n",
        "    'model_type':'reward_conditioned',\n",
        "}"
      ],
      "metadata": {
        "id": "3C8ay4GGDWI9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_embd = hparams['n_embd'] # embeding dimensionality, includes all heads\n",
        "        self.n_head = hparams['n_head'] #  num heads\n",
        "        self.block_size = hparams['block_size']\n",
        "        assert self.n_embd % self.n_head == 0\n",
        "\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.key = nn.Linear(self.n_embd, self.n_embd, bias=hparams['bias'])\n",
        "        self.query = nn.Linear(self.n_embd, self.n_embd, bias=hparams['bias'])\n",
        "        self.value = nn.Linear(self.n_embd, self.n_embd, bias=hparams['bias'])\n",
        "\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=hparams['bias'])\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(hparams['dropout'])\n",
        "        self.resid_dropout = nn.Dropout(hparams['dropout'])\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        # every token only comunicates with the previous ones\n",
        "        # This is typically used to register a buffer that should not to be considered a model parameter.\n",
        "        # creates -> self.mask\n",
        "        #self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size, self.block_size))\n",
        "        #                             .view(1, 1, self.block_size, self.block_size))\n",
        "        self.register_buffer(\"mask\", torch.tril(torch.ones(self.block_size + 1, self.block_size + 1))\n",
        "                                     .view(1, 1, self.block_size + 1, self.block_size + 1))\n",
        "\n",
        "        #self._reset_parameters() # uncomment if we need to initialize as the original transformer\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization\n",
        "        nn.init.xavier_uniform_(self.c_attn.weight)\n",
        "        nn.init.xavier_uniform_(self.c_proj.weight)\n",
        "        self.c_attn.bias.data.fill_(0)\n",
        "        self.c_proj.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        head_size = self.n_head, C // self.n_head\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2)  # (B, seqLen, numHeads, headSize) -> (B, numHeads, seqLen, headSize)\n",
        "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, numHeads, seqLen, headSize) x (B, numHeads, headSize, seqLen) -> (B, numHeads, seqLen, seqLen)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.mask[:,:,:T,:T] == 0, float('-inf')) # aplying the softmax -inf become 0\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        y = self.resid_dropout(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "2ikQ6v2l4xBW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MLP"
      ],
      "metadata": {
        "id": "SaTWkxbwPZIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2MLP(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_embd = hparams['n_embd']\n",
        "\n",
        "        self.c_fc    = nn.Linear(self.n_embd, 4 * self.n_embd, bias=hparams['bias']) # expand to dim*4\n",
        "        self.act    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * self.n_embd, self.n_embd, bias=hparams['bias'])\n",
        "        self.dropout = nn.Dropout(hparams['dropout'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.act(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "40LRBd5rPXgx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BLOCK\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://www.researchgate.net/publication/365625866/figure/fig2/AS:11431281098698218@1669051398448/Structure-of-the-applied-GPT-2-medium-architecture_W640.jpg\" width=\"280px\" alt=\"Zoom in to the Transformer\"/>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "3hovnlOXfBzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Block(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_embd = hparams['n_embd']\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(self.n_embd, bias=hparams['bias'])\n",
        "        self.attn = GPT2Attention()\n",
        "        self.ln_2 = nn.LayerNorm(self.n_embd, bias=hparams['bias'])\n",
        "        self.mlp = GPT2MLP()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x)) # LayerNorm -> attention -> Add ???? no hauria de layerNOrm després de l'attention?\n",
        "        x = x + self.mlp(self.ln_2(x)) # like x = self.ln_2(x + self.mlp(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "jaZwlkh4fGQQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TRANSFORMER style GTP2\n",
        "\n",
        "https://d2l.ai/_images/vit.svg"
      ],
      "metadata": {
        "id": "tmsL7TpX6l-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    \"\"\" GPT Language Model \"\"\"\n",
        "\n",
        "    def __init__(self, max_timestep):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block_size = hparams['block_size']\n",
        "        self.vocab_size = hparams['vocab_size']\n",
        "        self.n_embd = hparams['n_embd']\n",
        "        self.model_type = 'reward_conditioned'\n",
        "        self.max_timestep = max_timestep\n",
        "\n",
        "        # input embedding stem\n",
        "        self.tok_emb = nn.Embedding(self.vocab_size, self.n_embd)  # n_embd includes all heads -> tok embedding\n",
        "        # self.pos_emb = nn.Parameter(torch.zeros(1, config.block_size, config.n_embd))\n",
        "        self.pos_emb = nn.Parameter(torch.zeros(1, self.block_size + 1, self.n_embd))\n",
        "        self.global_pos_emb = nn.Parameter(torch.zeros(1, self.max_timestep+1, self.n_embd))\n",
        "        self.drop = nn.Dropout(hparams['dropout'])\n",
        "\n",
        "        # transformer\n",
        "        #self.blocks = nn.ModuleList([GPT2Block() for _ in range(hparams['n_layer'])])\n",
        "        self.blocks = nn.Sequential(*[GPT2Block() for _ in range(hparams['n_layer'])])\n",
        "\n",
        "        # decoder head\n",
        "        self.ln_f = nn.LayerNorm(self.n_embd)\n",
        "\n",
        "        # out linear\n",
        "        self.lm_head = nn.Linear(self.n_embd, self.vocab_size, bias=False)\n",
        "\n",
        "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "        # STATE encoding\n",
        "        self.state_encoder = nn.Sequential(\n",
        "            nn.Conv2d(4, 32, 8, stride=4, padding=0), # stack 4 frames -> 4 channel in\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(32, 64, 4, stride=2, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(64, 64, 3, stride=1, padding=0),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(3136, self.n_embd),\n",
        "            nn.Tanh())\n",
        "\n",
        "        # RETURN embedding\n",
        "        self.ret_emb = nn.Sequential(\n",
        "            nn.Linear(1, self.n_embd),\n",
        "            nn.Tanh())\n",
        "\n",
        "        # ACTION embedding\n",
        "        self.action_embeddings = nn.Sequential(\n",
        "            nn.Embedding(self.vocab_size, self.n_embd),\n",
        "            nn.Tanh())\n",
        "        # initialization\n",
        "        nn.init.normal_(self.action_embeddings[0].weight, mean=0.0, std=0.02)\n",
        "\n",
        "\n",
        "        # report number of parameters\n",
        "        n_params = sum(p.numel() for p in self.parameters())\n",
        "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
        "\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
        "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "\n",
        "    # state, action, and return\n",
        "    def forward(self, states, actions, targets=None, rtgs=None, timesteps=None):\n",
        "        # states: (batch, block_size, 4*84*84)\n",
        "        # actions: (batch, block_size, 1)\n",
        "        # targets: (batch, block_size, 1)\n",
        "        # rtgs: (batch, block_size, 1)\n",
        "        # timesteps: (batch, 1, 1)\n",
        "        #batch_size = states.shape[0]\n",
        "\n",
        "\n",
        "        # PLAN:\n",
        "        # 1. Compute embeddings for tokens\n",
        "        # pos_embedding = embed_t(t) # per-timestep (note: not per-token)\n",
        "        # s_embedding = embed_s(s) + pos_embedding\n",
        "        # a_embedding = embed_s(a) + pos_embedding\n",
        "        # R_embedding = embed_R(R) + pos_embedding\n",
        "\n",
        "        # STATE embedding\n",
        "        # TODO STACK*4 i gestionar blocks de 4 imatges x capturar moviment\n",
        "        state_embeddings = self.state_encoder(states.reshape(-1, 4, 84, 84).type(torch.float32).contiguous()) # (batch * block_size, n_embd)\n",
        "\n",
        "        #state_embeddings = self.state_encoder(states) # (batch * block_size, n_embd)\n",
        "        state_embeddings = state_embeddings.reshape(states.shape[0], states.shape[1], self.n_embd) # (batch, block_size, n_embd)\n",
        "\n",
        "        # create tokens\n",
        "        if actions is not None and self.model_type == 'reward_conditioned':\n",
        "            # RETURN & ACTION embeddings\n",
        "            rtg_embeddings = self.ret_emb(rtgs.type(torch.float32))\n",
        "            action_embeddings = self.action_embeddings(actions.type(torch.long).squeeze(-1)) # (batch, block_size, n_embd)\n",
        "\n",
        "            # token_embeddings inclou els 3 embeddings\n",
        "            token_embeddings = torch.zeros((states.shape[0], states.shape[1]*3 - int(targets is None), self.n_embd), dtype=torch.float32, device=state_embeddings.device)\n",
        "            token_embeddings[:,::3,:] = rtg_embeddings\n",
        "            token_embeddings[:,1::3,:] = state_embeddings\n",
        "            token_embeddings[:,2::3,:] = action_embeddings[:,-states.shape[1] + int(targets is None):,:]\n",
        "        elif actions is None and self.model_type == 'reward_conditioned': # only happens at very first timestep of evaluation\n",
        "            rtg_embeddings = self.ret_emb(rtgs.type(torch.float32))\n",
        "\n",
        "            token_embeddings = torch.zeros((states.shape[0], states.shape[1]*2, self.n_embd), dtype=torch.float32, device=state_embeddings.device)\n",
        "            token_embeddings[:,::2,:] = rtg_embeddings # really just [:,0,:]\n",
        "            token_embeddings[:,1::2,:] = state_embeddings # really just [:,1,:]\n",
        "        elif actions is not None and self.model_type == 'naive':\n",
        "            action_embeddings = self.action_embeddings(actions.type(torch.long).squeeze(-1)) # (batch, block_size, n_embd)\n",
        "\n",
        "            token_embeddings = torch.zeros((states.shape[0], states.shape[1]*2 - int(targets is None), self.n_embd), dtype=torch.float32, device=state_embeddings.device)\n",
        "            token_embeddings[:,::2,:] = state_embeddings\n",
        "            token_embeddings[:,1::2,:] = action_embeddings[:,-states.shape[1] + int(targets is None):,:]\n",
        "        elif actions is None and self.model_type == 'naive': # only happens at very first timestep of evaluation\n",
        "            token_embeddings = state_embeddings\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        # 2. interleave tokens as (R_1,s_1, a_1, ...., R_K, s_K)\n",
        "        # input_embeds = stack (R_embedding, s_embedding, a_embeding)\n",
        "        # position embeddings of shape (1, t=seq_len, n_embd)\n",
        "\n",
        "        all_global_pos_emb = torch.repeat_interleave(self.global_pos_emb, hparams['batch_size'], dim=0) # batch_size, traj_length, n_embd\n",
        "        position_embeddings = torch.gather(all_global_pos_emb, 1, torch.repeat_interleave(timesteps, self.n_embd, dim=-1)) + self.pos_emb[:, :token_embeddings.shape[1], :]\n",
        "\n",
        "\n",
        "        # 3. use transformer to get hidden states\n",
        "        # hidden_states = transformer(input_embeds=input_embeds)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        # token embeddings of shape (b, t=seq_len, n_embd)\n",
        "        # position embeddings of shape (1, t=seq_len, n_embd)\n",
        "        # 1 position embedding for every 3 tokens\n",
        "        x = self.drop(token_embeddings + position_embeddings)\n",
        "\n",
        "        x = self.blocks(x)\n",
        "\n",
        "        # 4. select hidden states for action prediction tokens\n",
        "        # a_hidden = unstack(hidden_states).actions\n",
        "        x = self.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        if actions is not None and self.model_type == 'reward_conditioned':\n",
        "            logits = logits[:, 1::3, :] # only keep predictions from state_embeddings\n",
        "        elif actions is None and self.model_type == 'reward_conditioned':\n",
        "            logits = logits[:, 1:, :]\n",
        "        elif actions is not None and self.model_type == 'naive':\n",
        "            logits = logits[:, ::2, :] # only keep predictions from state_embeddings\n",
        "        elif actions is None and self.model_type == 'naive':\n",
        "            logits = logits # for completeness\n",
        "        else:\n",
        "            raise NotImplementedError()\n",
        "\n",
        "        print('logits_type ', logits.dtype)\n",
        "        print('target_type ', targets.dtype)\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.reshape(-1, logits.size(-1)), targets.reshape(-1).long())\n",
        "\n",
        "        # 5. predict action\n",
        "        # return pred_a(a_hidden)\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    def get_block_size(self):\n",
        "        return self.block_size\n"
      ],
      "metadata": {
        "id": "1pDnX5-l6rSp"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATASET LOADING & TRAINING"
      ],
      "metadata": {
        "id": "pTP7OlDMtHBZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# training loop\n",
        "#for (R,s,a,t) in dataloader: # dims: (batch_size, K, dim)\n",
        "   # a_preds = DecisionTransformer(R,s,a,t)\n",
        "   #loss = mean((a_pred - a)**2)  # L2 loss for continuous actions\n",
        "   #optimizer.zero_grad()\n",
        "   #loss.backward()\n",
        "   #optimizer.step()\n",
        "\n",
        "# evaluation loop\n",
        "#target_return = 1 # for instance, expert-level return\n",
        "#R,s,a,t, done = `target_return], [env.reset()], [], [1], False\n",
        "#while not done: # autoregressive generation/sampling\n",
        "  # sample next action\n",
        "  #action = DecisionTransformer(R,s,a,t) [-1] # for cta actions\n",
        "  #new_s,r,done,_ = env.step(action)\n",
        "\n",
        "  # append new tokens to sequence\n",
        "  #R = R + [R[-1] - r] # decrease returns-to-go with reward\n",
        "  #s,q,t = s + [new_s], a +[action], t + [len(R)]\n",
        "  #R,s,a,t = R[-K:], ... # only keep context length of K"
      ],
      "metadata": {
        "id": "veLhztPW7Drt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset"
      ],
      "metadata": {
        "id": "8Hs-TmZOEEr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Atari dataset from https://console.cloud.google.com/storage/browser/atari-replay-datasets/dqn downloable with the command:\n",
        "#!gsutil -m cp -R gs://atari-replay-datasets/dqn/Pong/1/replay_logs .\n",
        "\n",
        "# But we get the dataset from my drive (Alex)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fIbvBdVEDpg",
        "outputId": "395dc6d2-aa03-43a5-cdd9-67d70dec4d24"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LOAD FILES TEST"
      ],
      "metadata": {
        "id": "In9VUJ8-Dwfs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#DATA LOADING"
      ],
      "metadata": {
        "id": "xnq031clD0D4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gzip\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from torch.utils.data import TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "# TRAINING PARAMS\n",
        "tparams = {\n",
        "    #'max_epochs':10,\n",
        "    'learning_rate':3e-4,\n",
        "    'betas':(0.9, 0.95),\n",
        "    'grad_norm_clip':1.0, #??????\n",
        "    #'weight_decay':0.1, # only applied on matmul weights\n",
        "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
        "    #'lr_decay':False,\n",
        "    #'warmup_tokens':375e6, # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
        "    #final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
        "}\n",
        "\n",
        "def load_file(filename):\n",
        "  with gzip.open(filename, 'rb') as f:\n",
        "    return np.load(f)\n",
        "\n",
        "def close_file(filename):\n",
        "  gzip.close(filename)\n",
        "\n",
        "\n",
        "def getData(game, max_size=500000):\n",
        "\n",
        "  action_files = sorted(list(Path(hparams['path']).glob(f\"dqn_Pong_5_replay_logs_$store$_action_ckpt.*.gz\")))\n",
        "  obs_files = sorted(list(Path(hparams['path']).glob(f\"dqn_Pong_5_replay_logs_$store$_observation_ckpt.*.gz\")))\n",
        "  reward_files = sorted(list(Path(hparams['path']).glob(f\"dqn_Pong_5_replay_logs_$store$_reward_ckpt.*.gz\")))\n",
        "  terminal_files = sorted(list(Path(hparams['path']).glob(f\"dqn_Pong_5_replay_logs_$store$_terminal_ckpt.*.gz\")))\n",
        "  #add_count_files = sorted(list(Path(hparams['path']).glob(f\"add_count_ckpt.*.gz\")))\n",
        "  #invalid_range_files = sorted(list(Path(hparams['path']).glob(f\"invalid_range_ckpt.*.gz\")))\n",
        "\n",
        "  obs_data = None\n",
        "  action_data = None\n",
        "  reward_data = None\n",
        "  terminal_data = None\n",
        "  rtg = None\n",
        "  timesteps = None\n",
        "\n",
        "  for obs_file,action_file,reward_file,terminal_file  in zip(obs_files, action_files, reward_files, terminal_files):\n",
        "\n",
        "      print(\"max_size, context \", max_size, hparams['context_length'])\n",
        "      print(\"****** loading data *********\")\n",
        "      #print(obs_file)\n",
        "      obs_data = load_file(obs_file)\n",
        "      obs_data = obs_data[:max_size*hparams['context_length']]\n",
        "\n",
        "      #print(action_file)\n",
        "      action_data = load_file(action_file)\n",
        "\n",
        "      action_data = action_data[:max_size*hparams['context_length']] # crop for memory\n",
        "      print(\"action_data_\", len(action_data))\n",
        "      #print(reward_file)\n",
        "      reward_data = load_file(reward_file)\n",
        "      reward_data = reward_data[:max_size*hparams['context_length']]\n",
        "      #print(terminal_file)\n",
        "      terminal_data = load_file(terminal_file)\n",
        "      terminal_data = terminal_data[:max_size*hparams['context_length']]\n",
        "      # array of terminal positions\n",
        "      terminal_pos = np.where(terminal_data==1)[0]\n",
        "      terminal_data = None # de-allocate mem\n",
        "      print(\"num episodes \", terminal_pos.shape)\n",
        "\n",
        "      # -- create reward-to-go dataset\n",
        "      start_index = 0\n",
        "      rtg = np.zeros_like(reward_data)\n",
        "      for i in terminal_pos:\n",
        "          curr_traj_returns = reward_data[start_index:i]\n",
        "          reward_acum = 0\n",
        "          for j in range(i-1, start_index-1, -1): # start from i-1\n",
        "              reward_acum += reward_data[j]\n",
        "              #rtg_j = curr_traj_returns[j-start_index:i-start_index]\n",
        "              rtg[j] = reward_acum\n",
        "          start_index = i\n",
        "      print('max rtg is %d' % max(rtg))\n",
        "\n",
        "      reward_data = None\n",
        "\n",
        "      # -- create timestep dataset ******************************\n",
        "      start_index = 0\n",
        "      timesteps = np.zeros(len(action_data), dtype=int)\n",
        "      for i in terminal_pos:\n",
        "          timesteps[start_index:i] = np.arange(i - start_index)\n",
        "          start_index = i\n",
        "\n",
        "      max_timestep = max(timesteps)\n",
        "      print('max timestep is %d' % max_timestep)\n",
        "      print(\"***** data loaded **********\")\n",
        "\n",
        "  print('action possible numbers: ', np.unique(action_data))\n",
        "  assert hparams['vocab_size'] == len(np.unique(action_data)), \"hparams['vocab_size'] should be the number of possible action values\"\n",
        "\n",
        "  # convert to tensors, grouping in sequences\n",
        "  obs_data = torch.tensor(obs_data).view(-1, hparams['context_length'],84, 84)\n",
        "  print(\"obs_\", obs_data.shape)\n",
        "  plt.imshow(obs_data[1000][0])\n",
        "  action_data = torch.tensor(action_data).view(-1, hparams['context_length'])\n",
        "  print(\"action_data_\", action_data.shape)\n",
        "  action_data = action_data\n",
        "  rtg = torch.tensor(rtg).view(-1, hparams['context_length'])\n",
        "  print(\"rtg_\", rtg.shape)\n",
        "  timesteps = torch.tensor(timesteps).view(-1, hparams['context_length'])\n",
        "  print(\"timesteps_\", timesteps.shape)\n",
        "\n",
        "  # dataLoader TODO test_data\n",
        "  #train_data,test_data, train_labels,test_labels = train_test_split(dataT, labelsT, test_size=.1)\n",
        "\n",
        "  dataset = TensorDataset(obs_data,action_data,rtg,timesteps)\n",
        "  dataloader = torch.utils.data.DataLoader(dataset,batch_size=hparams['batch_size'], shuffle=True)\n",
        "  return dataloader, max_timestep\n"
      ],
      "metadata": {
        "id": "VD4x7UIXtpWg"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GET DATA from disk\n",
        "dataLoader, maxTimestep = getData('Pong', max_size=17000) # 17000*30=510000 RAM mem limit\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "H5pWovvPIYw7",
        "outputId": "a902e091-96da-4cfa-d4fc-83dc48eedb3c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_size, context  17000 30\n",
            "****** loading data *********\n",
            "action_data_ 510000\n",
            "num episodes  (253,)\n",
            "max rtg is 21\n",
            "max timestep is 2821\n",
            "***** data loaded **********\n",
            "action possible numbers:  [0 1 2 3 4 5]\n",
            "obs_ torch.Size([17000, 30, 84, 84])\n",
            "action_data_ torch.Size([17000, 30])\n",
            "rtg_ torch.Size([17000, 30])\n",
            "timesteps_ torch.Size([17000, 30])\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print(reward_data[:1000])\n",
        "#print(rtg[:1000])\n",
        "#print(timesteps[:1000])\n",
        "#print(terminal_pos)\n",
        "#action_data[:1000]\n",
        "#print(terminal_pos)\n",
        "#np.unique(action_data)"
      ],
      "metadata": {
        "id": "2oreWPPzK3VH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#TRAINING"
      ],
      "metadata": {
        "id": "H0vfA0ytJrlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, dataloader):\n",
        "\n",
        "      losses = []\n",
        "\n",
        "      optimizer = torch.optim.AdamW(model.parameters(), lr=tparams['learning_rate'], betas=tparams['betas'])\n",
        "\n",
        "      lr = tparams['learning_rate']\n",
        "\n",
        "      # to store the current observation and the 3 previous ones\n",
        "      ob4 = torch.zeros((hparams['batch_size'], hparams['context_length'],4, 84, 84))\n",
        "\n",
        "      counter = 0\n",
        "\n",
        "      # TODO it's just 1 epoch, and just train_data\n",
        "      for ob,ac,r,t in dataloader:\n",
        "          # ob shape (batch, context_length,84, 84)\n",
        "\n",
        "          # STACK*4 we need to stack with the 3 previous observations -> reshape(-1, 4, 84, 84)\n",
        "          # TODO we have to reset ob4 in a new episode\n",
        "          ob = torch.unsqueeze(ob, 2) # (batch, context_len, 1, 84, 84 )\n",
        "          ob4[:,:,3,:] = ob [:,:,0,:]\n",
        "          ob = ob4\n",
        "\n",
        "          ob = ob.reshape(-1, hparams['context_length'], 4*84*84).type(torch.float32).contiguous()\n",
        "          ob = ob.float().to(device)\n",
        "          ac = torch.unsqueeze(ac, 2)\n",
        "          ac = ac.float().to(device)\n",
        "          r = torch.unsqueeze(r, 2)\n",
        "          r = r.float().to(device)\n",
        "          t = t[:,-1] # TODO Select the last timestep of the sequence ???\n",
        "          t = torch.unsqueeze(t,1)\n",
        "          t = torch.unsqueeze(t,2)\n",
        "          t = t.to(device)\n",
        "\n",
        "          model.train()\n",
        "          # logits, loss = model(x, y, r)\n",
        "          logits, loss = model(ob, ac, ac, r, t)\n",
        "          losses.append(loss.item())\n",
        "\n",
        "          # backprop and update the parameters\n",
        "          optimizer.zero_grad()\n",
        "          loss.backward()\n",
        "          torch.nn.utils.clip_grad_norm_(model.parameters(), tparams['grad_norm_clip'])\n",
        "          optimizer.step()\n",
        "\n",
        "          # decay the learning rate based on our progress\n",
        "          #if tparams['learning_decay']:\n",
        "          #    self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
        "          #    if self.tokens < tparams['warmup_tokens']:\n",
        "          #        # linear warmup\n",
        "          #        lr_mult = float(self.tokens) / float(max(1, tparams['warmup_tokens']))\n",
        "          #    else:\n",
        "          #        # cosine learning rate decay\n",
        "          #        progress = float(self.tokens - tparams['warmup_tokens']) / float(max(1, tparams['final_tokens'] - tparams['warmup_tokens']))\n",
        "          #        lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
        "          #    lr = tparams['learning_rate'] * lr_mult\n",
        "          #    for param_group in optimizer.param_groups:\n",
        "          #         param_group['lr'] = lr\n",
        "          #else:\n",
        "          #    lr = config.learning_rate\n",
        "\n",
        "          # update ob4, movint channels 2-4 to 0-3 (the 4rt will be the new state)\n",
        "          ob4[:,:,0:3,:] = ob4[:,:,1:4,:]\n",
        "\n",
        "          # report progress\n",
        "          counter += 1\n",
        "          if (counter + 1) % 1000 == 0:\n",
        "              print(f\"Loss {loss.item()}\")\n",
        "          #  print(f\"Loss ({counter+1}/{n_epochs}): {loss.item()}\")\n",
        "\n",
        "      obs_data = None\n",
        "      action_data = None\n",
        "      reward_data = None\n",
        "      terminal_data = None\n",
        "      dataset = None\n",
        "      dataloader = None\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "5BtYgwcMJpQ5"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#mconf = GPTConfig(vocab_size, hparams['block_size'],\n",
        "#                  n_layer=6, n_head=8, n_embd=128, model_type=PAR_model_type, max_timestep=max(timesteps))\n",
        "\n",
        "model_gpt = GPT(maxTimestep)\n",
        "#model_gpt = GPT(3000)\n",
        "\n",
        "train(model_gpt, dataLoader)\n",
        "\n",
        "# token_embeddings [2, 90, 128]\n",
        "# self.global_pos_emb [1, 1843, 128]\n",
        "# all_global_pos_emb [2, 1843, 128]\n",
        "#2_ torch.Size([2, 1, 128])\n",
        "#3_ torch.Size([1, 90, 128])\n",
        "# rtgs [2, 30, 1]\n",
        "# states [2, 30, 28224]\n",
        "# timesteps [2, 1, 1]\n",
        "# token_embeddings [2, 90, 128]\n",
        "# position_embeddings  [2, 90, 128]\n",
        "# self.pos_emb  [1, 91, 128]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "WgYhJAvXFDqC",
        "outputId": "2ecb2b66-fb65-4cde-ab34-1d144434dfd3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss 1.874470829963684\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.6566437482833862\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.8140872716903687\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.9932665824890137\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.924153447151184\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.647241473197937\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.690383791923523\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.709167242050171\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.817720890045166\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.6598427295684814\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.7232364416122437\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.779930591583252\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.8524413108825684\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.6325805187225342\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.7885810136795044\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.7690293788909912\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.7754690647125244\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.7547467947006226\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.8558857440948486\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.7245453596115112\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.8185724020004272\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.7582217454910278\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.8564521074295044\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.8080334663391113\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.768966555595398\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.7867499589920044\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.696981430053711\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.748776912689209\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.7854193449020386\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.671757459640503\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.8575254678726196\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.6818498373031616\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.6061292886734009\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.6702486276626587\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.981142282485962\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.8618645668029785\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.9585282802581787\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.7911971807479858\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.7131836414337158\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.634948492050171\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.7183489799499512\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.852301836013794\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.8009523153305054\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.8306572437286377\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.7992208003997803\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.7549004554748535\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n",
            "Loss 1.8576329946517944\n",
            "ob1_ torch.Size([2, 30, 1, 84, 84])\n",
            "ob_ torch.Size([2, 30, 28224])\n",
            "ac_ torch.Size([2, 30, 1])\n",
            "r_ torch.Size([2, 30, 1])\n",
            "t_ torch.Size([2, 1, 1])\n",
            "states_in  torch.Size([2, 30, 28224])\n",
            "rtgs  torch.Size([2, 30, 1])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "self.global_pos_emb  torch.Size([1, 2822, 128])\n",
            "all_global_pos_emb torch.Size([2, 2822, 128])\n",
            "2_ torch.Size([2, 1, 128])\n",
            "self.pos_emb  torch.Size([1, 91, 128])\n",
            "3_ torch.Size([1, 90, 128])\n",
            "token_embeddings  torch.Size([2, 90, 128])\n",
            "position_embeddings  torch.Size([2, 90, 128])\n",
            "logits_type  torch.float32\n",
            "target_type  torch.float32\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-16fe7a2df7ff>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#model_gpt = GPT(3000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_gpt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataLoader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# token_embeddings [2, 90, 128]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-fa98708203c0>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, dataloader)\u001b[0m\n\u001b[1;32m     48\u001b[0m           \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m           \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'grad_norm_clip'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m           \u001b[0;31m# decay the learning rate based on our progress\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m                             )\n\u001b[1;32m    372\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 373\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    374\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36m_use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'differentiable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dynamo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_break\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    182\u001b[0m             )\n\u001b[1;32m    183\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 184\u001b[0;31m             adamw(\n\u001b[0m\u001b[1;32m    185\u001b[0m                 \u001b[0mparams_with_grad\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m                 \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36madamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_single_tensor_adamw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     func(\n\u001b[0m\u001b[1;32m    336\u001b[0m         \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/optim/adamw.py\u001b[0m in \u001b[0;36m_single_tensor_adamw\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;31m# Perform stepweight decay\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GENERATE ACTIONS"
      ],
      "metadata": {
        "id": "VuQokT2kPE3W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_logits(logits, k):\n",
        "    v, ix = torch.topk(logits, k)\n",
        "    out = logits.clone()\n",
        "    out[out < v[:, [-1]]] = -float('Inf')\n",
        "    return out\n",
        "\n",
        "@torch.no_grad()\n",
        "def sample(model, x, steps, temperature=1.0, sample=False, top_k=None, actions=None, rtgs=None, timesteps=None):\n",
        "    \"\"\"\n",
        "    take a conditioning sequence of indices in x (of shape (b,t)) and predict the next token in\n",
        "    the sequence, feeding the predictions back into the model each time. Clearly the sampling\n",
        "    has quadratic complexity unlike an RNN that is only linear, and has a finite context window\n",
        "    of block_size, unlike an RNN that has an infinite context window.\n",
        "    \"\"\"\n",
        "    block_size = model.get_block_size()\n",
        "    model.eval()\n",
        "    for k in range(steps):\n",
        "        # x_cond = x if x.size(1) <= block_size else x[:, -block_size:] # crop context if needed\n",
        "        x_cond = x if x.size(1) <= block_size//3 else x[:, -block_size//3:] # crop context if needed\n",
        "        if actions is not None:\n",
        "            actions = actions if actions.size(1) <= block_size//3 else actions[:, -block_size//3:] # crop context if needed\n",
        "        rtgs = rtgs if rtgs.size(1) <= block_size//3 else rtgs[:, -block_size//3:] # crop context if needed\n",
        "        logits, _ = model(x_cond, actions=actions, targets=None, rtgs=rtgs, timesteps=timesteps)\n",
        "        # pluck the logits at the final step and scale by temperature\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "        # optionally crop probabilities to only the top k options\n",
        "        if top_k is not None:\n",
        "            logits = top_k_logits(logits, top_k)\n",
        "        # apply softmax to convert to probabilities\n",
        "        probs = F.softmax(logits, dim=-1)\n",
        "        # sample from the distribution or take the most likely\n",
        "        if sample:\n",
        "            ix = torch.multinomial(probs, num_samples=1)\n",
        "        else:\n",
        "            _, ix = torch.topk(probs, k=1, dim=-1)\n",
        "        # append to the sequence and continue\n",
        "        # x = torch.cat((x, ix), dim=1)\n",
        "        x = ix\n",
        "\n",
        "    return x\n",
        "\n"
      ],
      "metadata": {
        "id": "hhWDJ-7rcVQ4"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#GENERATE ACTIONS WITH THE MODEL"
      ],
      "metadata": {
        "id": "z6_UBw0gcp7s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install \"gym[accept-rom-license, atari]\""
      ],
      "metadata": {
        "id": "UqvmQXuSdszu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "#gym.envs.registration.registry.keys()"
      ],
      "metadata": {
        "id": "ZY_lF9_cdxeo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TEST the DT with GYM"
      ],
      "metadata": {
        "id": "lKE4iNWBfX6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"ALE/Breakout-v5\", new_step_api=True) # start the OpenAI gym breakout environment\n",
        "observation_image = env.reset() # get the image"
      ],
      "metadata": {
        "id": "fg8CQC-6dyn1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "\n",
        "  action = env.action_space.sample()\n",
        "  observation, reward, terminated, truncated, info = env.step(action)\n",
        "  #print(info)\n",
        "\n",
        "  if terminated or truncated:\n",
        "      break"
      ],
      "metadata": {
        "id": "xxLQlazdd8RQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot the state <observation)\n",
        "from PIL import Image\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "print(observation.shape)\n",
        "imgplot = plt.imshow(observation)"
      ],
      "metadata": {
        "id": "4cxlLQnCd89Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}