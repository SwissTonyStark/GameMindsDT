{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt"
      ],
      "metadata": {
        "id": "OPv4bFlo2-7G"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "HOViYCWJv16d"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformer architecture\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/transformer.png\" width=\"1000px\" alt=\"Zoom in to the Transformer\"/>\n",
        "</p>"
      ],
      "metadata": {
        "id": "wekMUPvt4Xdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ATTENTION"
      ],
      "metadata": {
        "id": "i_rPVTib4zNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# default GPT2 values?\n",
        "hparams = {\n",
        "    'n_layer':12, # num of blocks?\n",
        "    'n_head': 16, #\n",
        "    'n_embd':768, # 16 heads of size 64\n",
        "    'block_size':1024, # sequences of 1024 tokens\n",
        "    'dropout':0.1, # dropout value\n",
        "    'bias': True, # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster. warning! nn.LayerNorm doesn't support bias=False\n",
        "    'vocab_size':50257,\n",
        "}"
      ],
      "metadata": {
        "id": "3C8ay4GGDWI9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_embd = hparams['n_embd'] # embeding dimensionality, includes all heads\n",
        "        self.n_head = hparams['n_head'] #  num heads\n",
        "        self.block_size = hparams['block_size']\n",
        "        assert self.n_embd % self.n_head == 0\n",
        "\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(self.n_embd, 3 * self.n_embd, bias=hparams['bias'])\n",
        "\n",
        "        # output projection\n",
        "        self.c_out = nn.Linear(self.n_embd, self.n_embd, bias=hparams['bias'])\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(hparams['dropout'])\n",
        "        self.resid_dropout = nn.Dropout(hparams['dropout'])\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        # every token only comunicates with the previous ones\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(self.block_size, self.block_size))\n",
        "                                     .view(1, 1, self.block_size, self.block_size))\n",
        "\n",
        "        #self._reset_parameters() # uncomment if we need to initialize as the original transformer\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization\n",
        "        nn.init.xavier_uniform_(self.c_attn.weight)\n",
        "        nn.init.xavier_uniform_(self.c_out.weight)\n",
        "        self.c_attn.bias.data.fill_(0)\n",
        "        self.c_out.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        head_size = self.n_head, C // self.n_head\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, seqLen, numHeads, headSize) -> (B, numHeads, seqLen, headSize)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, seqLen, numHeads, headSize) -> (B, numHeads, seqLen, headSize)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, seqLen, numHeads, headSize) -> (B, numHeads, seqLen, headSize)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, numHeads, seqLen, headSize) x (B, numHeads, headSize, seqLen) -> (B, numHeads, seqLen, seqLen)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.tril[:,:,:T,:T] == 0, float('-inf')) # aplying the softmax -inf become 0\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.c_out(y)\n",
        "        y = self.resid_dropout(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "2ikQ6v2l4xBW"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# attention tester\n",
        "hparams = {\n",
        "    'batch_size':10,\n",
        "    'n_head': 2,\n",
        "    'n_embd':6, # 2 heads of size 3\n",
        "    'block_size':3, # seq_len 3 words\n",
        "    'dropout':0.1, # dropout value\n",
        "    'bias': True, # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "}\n",
        "\n",
        "x = torch.randn(hparams['batch_size'], hparams['block_size'], hparams['n_embd']) # batch size, sequence length, embedding dimensionality\n",
        "\n",
        "mha = CausalSelfAttention()\n",
        "optimizer = optim.Adam(mha.parameters())\n",
        "\n",
        "losses_mha = []\n",
        "n_epochs = 10000\n",
        "for i in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = mha(x)\n",
        "    loss = F.mse_loss(output, x) # Reconstruct input\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses_mha.append(loss.item())\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"Loss ({i+1}/{n_epochs}): {loss.item()}\")\n",
        "\n",
        "print(f\"\\nOutput:\\n{output[0,:3,:]}\\n\")\n",
        "print(f\"Query:\\n{x[0,:3,:]}\\n\")"
      ],
      "metadata": {
        "id": "lSrBG_d8SZ__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MLP"
      ],
      "metadata": {
        "id": "SaTWkxbwPZIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_embd = hparams['n_embd']\n",
        "\n",
        "        self.c_fc    = nn.Linear(self.n_embd, 4 * self.n_embd, bias=hparams['bias']) # expand to dim*4\n",
        "        self.gelu    = nn.GELU()\n",
        "        self.c_proj  = nn.Linear(4 * self.n_embd, self.n_embd, bias=hparams['bias'])\n",
        "        self.dropout = nn.Dropout(hparams['dropout'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "40LRBd5rPXgx"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP tester\n",
        "hparams = {\n",
        "    'batch_size':10,\n",
        "    'n_head': 2,\n",
        "    'n_embd':6, # 2 heads of size 3\n",
        "    'block_size':3, # seq_len 3 words\n",
        "    'dropout':0.1, # dropout value\n",
        "    'bias': True, # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "}\n",
        "\n",
        "x = torch.randn(hparams['batch_size'], hparams['block_size'], hparams['n_embd']) # batch size, sequence length, embedding dimensionality\n",
        "\n",
        "mlp = MLP()\n",
        "optimizer = optim.Adam(mlp.parameters())\n",
        "\n",
        "losses_mha = []\n",
        "n_epochs = 10000\n",
        "for i in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = mlp(x)\n",
        "    loss = F.mse_loss(output, x) # Reconstruct input\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses_mha.append(loss.item())\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"Loss ({i+1}/{n_epochs}): {loss.item()}\")\n",
        "\n",
        "print(\" \")\n",
        "print(\"input_shape \", x.shape)\n",
        "print(\"output_shape \", output.shape)\n",
        "\n",
        "print(f\"\\nOutput:\\n{output[0,:3,:]}\\n\")\n",
        "print(f\"Query:\\n{x[0,:3,:]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHDvQRlEcfnm",
        "outputId": "66edfb13-f595-4298-f24a-b89af0a35b55"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss (1000/10000): 0.19554820656776428\n",
            "Loss (2000/10000): 0.14630521833896637\n",
            "Loss (3000/10000): 0.07749444246292114\n",
            "Loss (4000/10000): 0.1958460956811905\n",
            "Loss (5000/10000): 0.09793464094400406\n",
            "Loss (6000/10000): 0.11632366478443146\n",
            "Loss (7000/10000): 0.13567720353603363\n",
            "Loss (8000/10000): 0.07998411357402802\n",
            "Loss (9000/10000): 0.20546944439411163\n",
            "Loss (10000/10000): 0.1160961166024208\n",
            " \n",
            "input_shape  torch.Size([10, 3, 6])\n",
            "output_shape  torch.Size([10, 3, 6])\n",
            "\n",
            "Output:\n",
            "tensor([[ 0.5046,  0.5136, -0.0833,  1.4909,  0.0000,  0.0000],\n",
            "        [-0.5315, -0.8549, -0.0436,  1.1264, -0.0000,  0.8436],\n",
            "        [ 1.5866, -0.6878, -1.6688, -1.1337,  0.6195,  1.1869]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "Query:\n",
            "tensor([[ 0.5048,  0.5135, -0.0837,  1.4903,  2.0361,  0.5054],\n",
            "        [-0.5319, -0.8551, -0.0436,  1.1264, -2.6470,  0.8448],\n",
            "        [ 1.5867, -0.6874, -1.6689, -1.1315,  0.6191,  1.1875]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BLOCK\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://www.researchgate.net/publication/365625866/figure/fig2/AS:11431281098698218@1669051398448/Structure-of-the-applied-GPT-2-medium-architecture_W640.jpg\" width=\"280px\" alt=\"Zoom in to the Transformer\"/>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "3hovnlOXfBzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Block(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_embd = hparams['n_embd']\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(self.n_embd, bias=hparams['bias'])\n",
        "        self.attn = CausalSelfAttention()\n",
        "        self.ln_2 = nn.LayerNorm(self.n_embd, bias=hparams['bias'])\n",
        "        self.mlp = MLP()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x)) # LayerNorm -> attention -> Add ???? no hauria de layerNOrm després de l'attention?\n",
        "        x = x + self.mlp(self.ln_2(x)) # like x = self.ln_2(x + self.mlp(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "jaZwlkh4fGQQ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block tester\n",
        "hparams = {\n",
        "    'batch_size':10,\n",
        "    'n_head': 2,\n",
        "    'n_embd':6, # 2 heads of size 3\n",
        "    'block_size':3, # seq_len 3 words\n",
        "    'dropout':0.1, # dropout value\n",
        "    'bias': True, # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "}\n",
        "\n",
        "x = torch.randn(hparams['batch_size'], hparams['block_size'], hparams['n_embd']) # batch size, sequence length, embedding dimensionality\n",
        "\n",
        "blk = GPT2Block()\n",
        "optimizer = optim.Adam(blk.parameters())\n",
        "\n",
        "losses_mha = []\n",
        "n_epochs = 10000\n",
        "for i in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = blk(x)\n",
        "    loss = F.mse_loss(output, x) # Reconstruct input\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses_mha.append(loss.item())\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"Loss ({i+1}/{n_epochs}): {loss.item()}\")\n",
        "\n",
        "print(\" \")\n",
        "print(\"input_shape \", x.shape)\n",
        "print(\"output_shape \", output.shape)\n",
        "\n",
        "print(f\"\\nOutput:\\n{output[0,:3,:]}\\n\")\n",
        "print(f\"Query:\\n{x[0,:3,:]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2yv2Lb502fG",
        "outputId": "c0553771-1b24-4f13-afb3-fea10d49d1ea"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss (1000/10000): 0.0001341675379080698\n",
            "Loss (2000/10000): 1.9141029042657465e-05\n",
            "Loss (3000/10000): 5.915315341553651e-06\n",
            "Loss (4000/10000): 1.6615333606750937e-06\n",
            "Loss (5000/10000): 8.235734867412248e-07\n",
            "Loss (6000/10000): 4.255325336544047e-07\n",
            "Loss (7000/10000): 2.427399863336177e-07\n",
            "Loss (8000/10000): 1.7640056171330798e-07\n",
            "Loss (9000/10000): 1.1159019663864456e-07\n",
            "Loss (10000/10000): 2.3057913267621188e-07\n",
            " \n",
            "input_shape  torch.Size([10, 3, 6])\n",
            "output_shape  torch.Size([10, 3, 6])\n",
            "\n",
            "Output:\n",
            "tensor([[-1.3583, -0.7305,  1.3442,  1.2064,  2.0346, -0.4798],\n",
            "        [-1.5790, -0.6084, -0.0171,  0.9759, -0.4144, -0.3798],\n",
            "        [ 0.8599, -0.1505,  0.3228,  0.4175, -0.7545,  1.2290]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "Query:\n",
            "tensor([[-1.3579, -0.7306,  1.3436,  1.2054,  2.0343, -0.4795],\n",
            "        [-1.5795, -0.6085, -0.0176,  0.9756, -0.4143, -0.3797],\n",
            "        [ 0.8600, -0.1500,  0.3233,  0.4166, -0.7548,  1.2287]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TRANSFORMER style GTP2"
      ],
      "metadata": {
        "id": "tmsL7TpX6l-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    \"\"\" GPT Language Model \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block_size = hparams['block_size']\n",
        "        self.vocab_size = hparams['vocab_size']\n",
        "        self.n_embd = hparams['n_embd']\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(self.vocab_size, self.n_embd), # n_embd includes all heads\n",
        "            wpe = nn.Embedding(self.block_size, self.n_embd),\n",
        "            drop = nn.Dropout(hparams['dropout']),\n",
        "            h = nn.ModuleList([GPT2Block() for _ in range(hparams['n_layer'])]),\n",
        "            ln_f = nn.LayerNorm(self.n_embd),\n",
        "        ))\n",
        "        # out linear\n",
        "        self.lm_head = nn.Linear(self.n_embd, self.vocab_size, bias=False)\n",
        "\n",
        "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * hparams['n_layer']))\n",
        "\n",
        "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size() # (B, seq_len)\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "        # position\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t=seq_len)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t=seq_len, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t=seq_len, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # either sample from the distribution or take the most likely element\n",
        "            if do_sample:\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "1pDnX5-l6rSp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT tester\n",
        "hparams = {\n",
        "    'n_layer':12, # num of blocks?\n",
        "    'batch_size':10,\n",
        "    'n_head': 2,\n",
        "    'n_embd':6, # 2 heads of size 3\n",
        "    'block_size':3, # seq_len 3 words\n",
        "    'dropout':0.1, # dropout value\n",
        "    'bias': True, # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "    'vocab_size':50257,\n",
        "}\n",
        "\n",
        "x = torch.randint(10, (hparams['batch_size'], hparams['block_size'])) # batch size, sequence length\n",
        "\n",
        "model = GPT()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "losses_mha = []\n",
        "n_epochs = 10000\n",
        "for i in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output, loss = model(x, x) # calculate loss internally\n",
        "\n",
        "#    loss = F.mse_loss(output, x) # Reconstruct input\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses_mha.append(loss.item())\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"Loss ({i+1}/{n_epochs}): {loss.item()}\")\n",
        "\n",
        "print(\" \")\n",
        "print(\"input_shape \", x.shape)\n",
        "print(\"output_shape \", output.shape)\n",
        "\n",
        "print(f\"\\nOutput:\\n{output[0,:3,:]}\\n\")\n",
        "print(f\"Query:\\n{x[0,:3]}\\n\")"
      ],
      "metadata": {
        "id": "vst9zL-8BO-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## compare with HF GPT2"
      ],
      "metadata": {
        "id": "GekJPmdzuiO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtGUMaY5FiG4",
        "outputId": "4c45b95b-28dc-449a-d199-ed22e5cf9c34"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize a pretrained GPT model by copying over the weights\n",
        "#from a huggingface/transformers checkpoint.\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# init a huggingface/transformers model\n",
        "model_hf = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "print(\"HF GPT************** \\n\", model_hf)\n",
        "\n",
        "hparams = {\n",
        "    'n_layer':12, # num of blocks?\n",
        "    'batch_size':1,\n",
        "    'n_head': 12,\n",
        "    'n_embd':768, # 12 heads of size 64\n",
        "    'block_size':1024, # seq_len 3 words\n",
        "    'dropout':0.1, # dropout value\n",
        "    'bias': True, # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "    'vocab_size':50257,\n",
        "}\n",
        "\n",
        "# init our transformer model\n",
        "model = GPT()\n",
        "print(\"our GPT************** \\n\", model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXmqjUOuGNXX",
        "outputId": "63410047-7bce-418b-fb31-b6d9730f0c18"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF GPT************** \n",
            " GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n",
            "number of parameters: 124.44M\n",
            "our GPT************** \n",
            " GPT(\n",
            "  (transformer): ModuleDict(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): CausalSelfAttention(\n",
            "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (c_out): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): MLP(\n",
            "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (gelu): GELU(approximate='none')\n",
            "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sd_hf = model_hf.state_dict()\n",
        "sd = model.state_dict()\n",
        "\n",
        "sd_keys = sd.keys()\n",
        "sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
        "\n",
        "\n",
        "\n",
        "# copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "sd_keys_hf = sd_hf.keys()\n",
        "sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "# basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "# this means that we have to transpose these weights when we import them\n",
        "assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "for k in sd_keys_hf:\n",
        "    if any(k.endswith(w) for w in transposed):\n",
        "        # special treatment for the Conv1D weights we need to transpose\n",
        "        assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "        with torch.no_grad():\n",
        "            sd[k].copy_(sd_hf[k].t())\n",
        "    else:\n",
        "        # vanilla copy over the other parameters\n",
        "        assert sd_hf[k].shape == sd[k].shape\n",
        "        with torch.no_grad():\n",
        "            sd[k].copy_(sd_hf[k])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552,
          "referenced_widgets": [
            "ecdd599dce7d4dc1b4f0ad49e85d7292",
            "3ec55a06274e4808a027fbbfbc653553",
            "205ddb096c304880a94c0b8f46d15082",
            "75833b4d550444de9a1d545cb5681855",
            "0b0e5bc53f654c5593146c7ffcbdf355",
            "ebf70ecfb1f64a73bb24689d9575669b",
            "40c1dfc4168248df988e5745bba60be0",
            "023d801d3c424b63b967852cbc92cc87",
            "4ed4d8092dd449549ab2bd997aa640b5",
            "17c2a1e124d644c291b357678960904c",
            "1b845477f28440978855f28ae09f1124",
            "21ff511da09f4601ba3a20e117c897a2",
            "1f2d55f1458d465eaffd8f9c323bf82c",
            "6bf4130f17a04bc2a74026706e052935",
            "f834e0ac01bf412da083e39001e04381",
            "e90ac5d019c74b70ab014805597a4b84",
            "96cece6cf3b84c168b622fc9917d57ee",
            "4400770f51994a369ec7ac17834b54dd",
            "d38198859c30432ab63ffb431a1137c1",
            "234d60d2430b45ed847b05a43f37165a",
            "ecc532a8941f48a696f636081a55851b",
            "ad601b31d6bc4ca2bc35318320392b05",
            "796c5f83e408420b863254aa5ed67281",
            "d905018a4e2442b491cf7f29eea895aa",
            "a3510d62673b4aa8bd0c52dcc4d7888d",
            "712bea77bc44424c83921d12a7b81551",
            "158b8637931344cdafa3a7a69e24c889",
            "b055186bb04a42c5af56af9740826e23",
            "9c0545480fb14b01a37c2cfcfce74766",
            "f37f5909c6d0434b9bd5be5c5106190a",
            "a72abeee0819446db9f17e41e63fcfd4",
            "bc3e7e37191a4690afb25bba855a1595",
            "823491bb9bee49c1b625016a44df139b"
          ]
        },
        "id": "6I8W9paWvA1K",
        "outputId": "2e936d33-cee0-4dfe-d1e3-71c12774b084"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading weights from pretrained gpt: gpt2\n",
            "{'n_layer': 12, 'n_head': 12, 'n_embd': 768}\n",
            "forcing vocab_size=50257, block_size=1024, bias=True\n",
            "overriding dropout rate to 0.1\n",
            "number of parameters: 2.64M\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ecdd599dce7d4dc1b4f0ad49e85d7292"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "21ff511da09f4601ba3a20e117c897a2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "796c5f83e408420b863254aa5ed67281"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "mismatched keys: 149 != 44",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-314ffba4eee0>\u001b[0m in \u001b[0;36m<cell line: 44>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;31m# basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;31m# this means that we have to transpose these weights when we import them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd_keys_hf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msd_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msd_keys_hf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtransposed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: mismatched keys: 149 != 44"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": []
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ecdd599dce7d4dc1b4f0ad49e85d7292": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3ec55a06274e4808a027fbbfbc653553",
              "IPY_MODEL_205ddb096c304880a94c0b8f46d15082",
              "IPY_MODEL_75833b4d550444de9a1d545cb5681855"
            ],
            "layout": "IPY_MODEL_0b0e5bc53f654c5593146c7ffcbdf355"
          }
        },
        "3ec55a06274e4808a027fbbfbc653553": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebf70ecfb1f64a73bb24689d9575669b",
            "placeholder": "​",
            "style": "IPY_MODEL_40c1dfc4168248df988e5745bba60be0",
            "value": "config.json: 100%"
          }
        },
        "205ddb096c304880a94c0b8f46d15082": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_023d801d3c424b63b967852cbc92cc87",
            "max": 665,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4ed4d8092dd449549ab2bd997aa640b5",
            "value": 665
          }
        },
        "75833b4d550444de9a1d545cb5681855": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17c2a1e124d644c291b357678960904c",
            "placeholder": "​",
            "style": "IPY_MODEL_1b845477f28440978855f28ae09f1124",
            "value": " 665/665 [00:00&lt;00:00, 37.5kB/s]"
          }
        },
        "0b0e5bc53f654c5593146c7ffcbdf355": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ebf70ecfb1f64a73bb24689d9575669b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40c1dfc4168248df988e5745bba60be0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "023d801d3c424b63b967852cbc92cc87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4ed4d8092dd449549ab2bd997aa640b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "17c2a1e124d644c291b357678960904c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b845477f28440978855f28ae09f1124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "21ff511da09f4601ba3a20e117c897a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f2d55f1458d465eaffd8f9c323bf82c",
              "IPY_MODEL_6bf4130f17a04bc2a74026706e052935",
              "IPY_MODEL_f834e0ac01bf412da083e39001e04381"
            ],
            "layout": "IPY_MODEL_e90ac5d019c74b70ab014805597a4b84"
          }
        },
        "1f2d55f1458d465eaffd8f9c323bf82c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96cece6cf3b84c168b622fc9917d57ee",
            "placeholder": "​",
            "style": "IPY_MODEL_4400770f51994a369ec7ac17834b54dd",
            "value": "model.safetensors: 100%"
          }
        },
        "6bf4130f17a04bc2a74026706e052935": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d38198859c30432ab63ffb431a1137c1",
            "max": 548105171,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_234d60d2430b45ed847b05a43f37165a",
            "value": 548105171
          }
        },
        "f834e0ac01bf412da083e39001e04381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecc532a8941f48a696f636081a55851b",
            "placeholder": "​",
            "style": "IPY_MODEL_ad601b31d6bc4ca2bc35318320392b05",
            "value": " 548M/548M [00:05&lt;00:00, 140MB/s]"
          }
        },
        "e90ac5d019c74b70ab014805597a4b84": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96cece6cf3b84c168b622fc9917d57ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4400770f51994a369ec7ac17834b54dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d38198859c30432ab63ffb431a1137c1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "234d60d2430b45ed847b05a43f37165a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ecc532a8941f48a696f636081a55851b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad601b31d6bc4ca2bc35318320392b05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "796c5f83e408420b863254aa5ed67281": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d905018a4e2442b491cf7f29eea895aa",
              "IPY_MODEL_a3510d62673b4aa8bd0c52dcc4d7888d",
              "IPY_MODEL_712bea77bc44424c83921d12a7b81551"
            ],
            "layout": "IPY_MODEL_158b8637931344cdafa3a7a69e24c889"
          }
        },
        "d905018a4e2442b491cf7f29eea895aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b055186bb04a42c5af56af9740826e23",
            "placeholder": "​",
            "style": "IPY_MODEL_9c0545480fb14b01a37c2cfcfce74766",
            "value": "generation_config.json: 100%"
          }
        },
        "a3510d62673b4aa8bd0c52dcc4d7888d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f37f5909c6d0434b9bd5be5c5106190a",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a72abeee0819446db9f17e41e63fcfd4",
            "value": 124
          }
        },
        "712bea77bc44424c83921d12a7b81551": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc3e7e37191a4690afb25bba855a1595",
            "placeholder": "​",
            "style": "IPY_MODEL_823491bb9bee49c1b625016a44df139b",
            "value": " 124/124 [00:00&lt;00:00, 5.61kB/s]"
          }
        },
        "158b8637931344cdafa3a7a69e24c889": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b055186bb04a42c5af56af9740826e23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c0545480fb14b01a37c2cfcfce74766": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f37f5909c6d0434b9bd5be5c5106190a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a72abeee0819446db9f17e41e63fcfd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc3e7e37191a4690afb25bba855a1595": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "823491bb9bee49c1b625016a44df139b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
