{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pylab as plt"
      ],
      "metadata": {
        "id": "OPv4bFlo2-7G"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "HOViYCWJv16d"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Â Transformer architecture\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://lilianweng.github.io/lil-log/assets/images/transformer.png\" width=\"1000px\" alt=\"Zoom in to the Transformer\"/>\n",
        "</p>"
      ],
      "metadata": {
        "id": "wekMUPvt4Xdt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ATTENTION"
      ],
      "metadata": {
        "id": "i_rPVTib4zNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# default GPT2 values?\n",
        "hparams = {\n",
        "    'n_layer':12, # num of blocks?\n",
        "    'n_head': 16, #\n",
        "    'n_embd':768, # 16 heads of size 64\n",
        "    'block_size':1024, # sequences of 1024 tokens\n",
        "    'dropout':0.1, # dropout value\n",
        "    'bias': True, # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster. warning! nn.LayerNorm doesn't support bias=False\n",
        "    'vocab_size':50257,\n",
        "}"
      ],
      "metadata": {
        "id": "3C8ay4GGDWI9"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    A vanilla multi-head masked self-attention layer with a projection at the end.\n",
        "    It is possible to use torch.nn.MultiheadAttention\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_embd = hparams['n_embd'] # embeding dimensionality, includes all heads\n",
        "        self.n_head = hparams['n_head'] #  num heads\n",
        "        self.block_size = hparams['block_size']\n",
        "        assert self.n_embd % self.n_head == 0\n",
        "\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(self.n_embd, 3 * self.n_embd, bias=hparams['bias'])\n",
        "\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(self.n_embd, self.n_embd, bias=hparams['bias'])\n",
        "        # regularization\n",
        "        self.attn_dropout = nn.Dropout(hparams['dropout'])\n",
        "        self.resid_dropout = nn.Dropout(hparams['dropout'])\n",
        "        # causal mask to ensure that attention is only applied to the left in the input sequence\n",
        "        # every token only comunicates with the previous ones\n",
        "        self.register_buffer(\"tril\", torch.tril(torch.ones(self.block_size, self.block_size))\n",
        "                                     .view(1, 1, self.block_size, self.block_size))\n",
        "\n",
        "        self._reset_parameters() # uncomment if we need to initialize as the original transformer\n",
        "\n",
        "    def _reset_parameters(self):\n",
        "        # Original Transformer initialization\n",
        "        nn.init.xavier_uniform_(self.c_attn.weight)\n",
        "        nn.init.xavier_uniform_(self.c_proj.weight)\n",
        "        self.c_attn.bias.data.fill_(0)\n",
        "        self.c_proj.bias.data.fill_(0)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        head_size = self.n_head, C // self.n_head\n",
        "\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        q, k ,v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
        "\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, seqLen, numHeads, headSize) -> (B, numHeads, seqLen, headSize)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, seqLen, numHeads, headSize) -> (B, numHeads, seqLen, headSize)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, seqLen, numHeads, headSize) -> (B, numHeads, seqLen, headSize)\n",
        "\n",
        "        # causal self-attention; Self-attend: (B, numHeads, seqLen, headSize) x (B, numHeads, headSize, seqLen) -> (B, numHeads, seqLen, seqLen)\n",
        "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
        "        att = att.masked_fill(self.tril[:,:,:T,:T] == 0, float('-inf')) # aplying the softmax -inf become 0\n",
        "        att = F.softmax(att, dim=-1)\n",
        "        att = self.attn_dropout(att)\n",
        "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        y = self.resid_dropout(y)\n",
        "        return y"
      ],
      "metadata": {
        "id": "2ikQ6v2l4xBW"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# attention tester\n",
        "hparams = {\n",
        "    'batch_size':10,\n",
        "    'n_head': 2,\n",
        "    'n_embd':6, # 2 heads of size 3\n",
        "    'block_size':3, # seq_len 3 words\n",
        "    'dropout':0.1, # dropout value\n",
        "    'bias': True, # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "}\n",
        "\n",
        "x = torch.randn(hparams['batch_size'], hparams['block_size'], hparams['n_embd']) # batch size, sequence length, embedding dimensionality\n",
        "\n",
        "mha = GPT2Attention()\n",
        "optimizer = optim.Adam(mha.parameters())\n",
        "\n",
        "losses_mha = []\n",
        "n_epochs = 10000\n",
        "for i in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = mha(x)\n",
        "    loss = F.mse_loss(output, x) # Reconstruct input\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses_mha.append(loss.item())\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"Loss ({i+1}/{n_epochs}): {loss.item()}\")\n",
        "\n",
        "print(f\"\\nOutput:\\n{output[0,:3,:]}\\n\")\n",
        "print(f\"Query:\\n{x[0,:3,:]}\\n\")"
      ],
      "metadata": {
        "id": "lSrBG_d8SZ__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##MLP"
      ],
      "metadata": {
        "id": "SaTWkxbwPZIV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2MLP(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_embd = hparams['n_embd']\n",
        "\n",
        "        self.c_fc    = nn.Linear(self.n_embd, 4 * self.n_embd, bias=hparams['bias']) # expand to dim*4\n",
        "        self.c_proj  = nn.Linear(4 * self.n_embd, self.n_embd, bias=hparams['bias'])\n",
        "        self.act    = nn.GELU()\n",
        "        self.dropout = nn.Dropout(hparams['dropout'])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.act(x)\n",
        "        x = self.c_proj(x)\n",
        "        x = self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "40LRBd5rPXgx"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# MLP tester\n",
        "hparams = {\n",
        "    'batch_size':10,\n",
        "    'n_head': 2,\n",
        "    'n_embd':6, # 2 heads of size 3\n",
        "    'block_size':3, # seq_len 3 words\n",
        "    'dropout':0.1, # dropout value\n",
        "    'bias': True, # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "}\n",
        "\n",
        "x = torch.randn(hparams['batch_size'], hparams['block_size'], hparams['n_embd']) # batch size, sequence length, embedding dimensionality\n",
        "\n",
        "mlp = GPT2MLP()\n",
        "optimizer = optim.Adam(mlp.parameters())\n",
        "\n",
        "losses_mha = []\n",
        "n_epochs = 10000\n",
        "for i in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = mlp(x)\n",
        "    loss = F.mse_loss(output, x) # Reconstruct input\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses_mha.append(loss.item())\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"Loss ({i+1}/{n_epochs}): {loss.item()}\")\n",
        "\n",
        "print(\" \")\n",
        "print(\"input_shape \", x.shape)\n",
        "print(\"output_shape \", output.shape)\n",
        "\n",
        "print(f\"\\nOutput:\\n{output[0,:3,:]}\\n\")\n",
        "print(f\"Query:\\n{x[0,:3,:]}\\n\")"
      ],
      "metadata": {
        "id": "FHDvQRlEcfnm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##BLOCK\n",
        "\n",
        "\n",
        "<p align=\"center\">\n",
        "<img src=\"https://www.researchgate.net/publication/365625866/figure/fig2/AS:11431281098698218@1669051398448/Structure-of-the-applied-GPT-2-medium-architecture_W640.jpg\" width=\"280px\" alt=\"Zoom in to the Transformer\"/>\n",
        "</p>\n"
      ],
      "metadata": {
        "id": "3hovnlOXfBzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT2Block(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_embd = hparams['n_embd']\n",
        "\n",
        "        self.ln_1 = nn.LayerNorm(self.n_embd, bias=hparams['bias'])\n",
        "        self.attn = GPT2Attention()\n",
        "        self.ln_2 = nn.LayerNorm(self.n_embd, bias=hparams['bias'])\n",
        "        self.mlp = GPT2MLP()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x)) # LayerNorm -> attention -> Add ???? no hauria de layerNOrm desprÃ©s de l'attention?\n",
        "        x = x + self.mlp(self.ln_2(x)) # like x = self.ln_2(x + self.mlp(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "jaZwlkh4fGQQ"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Block tester\n",
        "hparams = {\n",
        "    'batch_size':10,\n",
        "    'n_head': 2,\n",
        "    'n_embd':6, # 2 heads of size 3\n",
        "    'block_size':3, # seq_len 3 words\n",
        "    'dropout':0.1, # dropout value\n",
        "    'bias': True, # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "}\n",
        "\n",
        "x = torch.randn(hparams['batch_size'], hparams['block_size'], hparams['n_embd']) # batch size, sequence length, embedding dimensionality\n",
        "\n",
        "blk = GPT2Block()\n",
        "optimizer = optim.Adam(blk.parameters())\n",
        "\n",
        "losses_mha = []\n",
        "n_epochs = 10000\n",
        "for i in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output = blk(x)\n",
        "    loss = F.mse_loss(output, x) # Reconstruct input\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses_mha.append(loss.item())\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"Loss ({i+1}/{n_epochs}): {loss.item()}\")\n",
        "\n",
        "print(\" \")\n",
        "print(\"input_shape \", x.shape)\n",
        "print(\"output_shape \", output.shape)\n",
        "\n",
        "print(f\"\\nOutput:\\n{output[0,:3,:]}\\n\")\n",
        "print(f\"Query:\\n{x[0,:3,:]}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q2yv2Lb502fG",
        "outputId": "c0553771-1b24-4f13-afb3-fea10d49d1ea"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss (1000/10000): 0.0001341675379080698\n",
            "Loss (2000/10000): 1.9141029042657465e-05\n",
            "Loss (3000/10000): 5.915315341553651e-06\n",
            "Loss (4000/10000): 1.6615333606750937e-06\n",
            "Loss (5000/10000): 8.235734867412248e-07\n",
            "Loss (6000/10000): 4.255325336544047e-07\n",
            "Loss (7000/10000): 2.427399863336177e-07\n",
            "Loss (8000/10000): 1.7640056171330798e-07\n",
            "Loss (9000/10000): 1.1159019663864456e-07\n",
            "Loss (10000/10000): 2.3057913267621188e-07\n",
            " \n",
            "input_shape  torch.Size([10, 3, 6])\n",
            "output_shape  torch.Size([10, 3, 6])\n",
            "\n",
            "Output:\n",
            "tensor([[-1.3583, -0.7305,  1.3442,  1.2064,  2.0346, -0.4798],\n",
            "        [-1.5790, -0.6084, -0.0171,  0.9759, -0.4144, -0.3798],\n",
            "        [ 0.8599, -0.1505,  0.3228,  0.4175, -0.7545,  1.2290]],\n",
            "       grad_fn=<SliceBackward0>)\n",
            "\n",
            "Query:\n",
            "tensor([[-1.3579, -0.7306,  1.3436,  1.2054,  2.0343, -0.4795],\n",
            "        [-1.5795, -0.6085, -0.0176,  0.9756, -0.4143, -0.3797],\n",
            "        [ 0.8600, -0.1500,  0.3233,  0.4166, -0.7548,  1.2287]])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TRANSFORMER style GTP2"
      ],
      "metadata": {
        "id": "tmsL7TpX6l-a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GPT(nn.Module):\n",
        "    \"\"\" GPT Language Model \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "        self.block_size = hparams['block_size']\n",
        "        self.vocab_size = hparams['vocab_size']\n",
        "        self.n_embd = hparams['n_embd']\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(self.vocab_size, self.n_embd), # n_embd includes all heads\n",
        "            wpe = nn.Embedding(self.block_size, self.n_embd),\n",
        "            drop = nn.Dropout(hparams['dropout']),\n",
        "            h = nn.ModuleList([GPT2Block() for _ in range(hparams['n_layer'])]),\n",
        "            ln_f = nn.LayerNorm(self.n_embd),\n",
        "        ))\n",
        "        # out linear\n",
        "        self.lm_head = nn.Linear(self.n_embd, self.vocab_size, bias=False)\n",
        "\n",
        "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
        "        self.apply(self._init_weights)\n",
        "        for pn, p in self.named_parameters():\n",
        "            if pn.endswith('c_proj.weight'):\n",
        "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * hparams['n_layer']))\n",
        "\n",
        "        # report number of parameters (note we don't count the decoder parameters in lm_head)\n",
        "        n_params = sum(p.numel() for p in self.transformer.parameters())\n",
        "        print(\"number of parameters: %.2fM\" % (n_params/1e6,))\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "        elif isinstance(module, nn.LayerNorm):\n",
        "            torch.nn.init.zeros_(module.bias)\n",
        "            torch.nn.init.ones_(module.weight)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        device = idx.device\n",
        "        b, t = idx.size() # (B, seq_len)\n",
        "        assert t <= self.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.block_size}\"\n",
        "        # position\n",
        "        pos = torch.arange(0, t, dtype=torch.long, device=device).unsqueeze(0) # shape (1, t=seq_len)\n",
        "\n",
        "        # forward the GPT model itself\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t=seq_len, n_embd)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (1, t=seq_len, n_embd)\n",
        "        x = self.transformer.drop(tok_emb + pos_emb)\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x)\n",
        "\n",
        "        # if we are given some desired targets also calculate the loss\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def generate(self, idx, max_new_tokens, temperature=1.0, do_sample=False, top_k=None):\n",
        "        \"\"\"\n",
        "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
        "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
        "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
        "        \"\"\"\n",
        "        for _ in range(max_new_tokens):\n",
        "            # if the sequence context is growing too long we must crop it at block_size\n",
        "            idx_cond = idx if idx.size(1) <= self.block_size else idx[:, -self.block_size:]\n",
        "            # forward the model to get the logits for the index in the sequence\n",
        "            logits, _ = self(idx_cond)\n",
        "            # pluck the logits at the final step and scale by desired temperature\n",
        "            logits = logits[:, -1, :] / temperature\n",
        "            # optionally crop the logits to only the top k options\n",
        "            if top_k is not None:\n",
        "                v, _ = torch.topk(logits, top_k)\n",
        "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
        "            # apply softmax to convert logits to (normalized) probabilities\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            # either sample from the distribution or take the most likely element\n",
        "            if do_sample:\n",
        "                idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            else:\n",
        "                _, idx_next = torch.topk(probs, k=1, dim=-1)\n",
        "            # append sampled index to the running sequence and continue\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "1pDnX5-l6rSp"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GPT tester\n",
        "hparams = {\n",
        "    'n_layer':12, # num of blocks?\n",
        "    'batch_size':10,\n",
        "    'n_head': 2,\n",
        "    'n_embd':6, # 2 heads of size 3\n",
        "    'block_size':3, # seq_len 3 words\n",
        "    'dropout':0.1, # dropout value\n",
        "    'bias': True, # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "    'vocab_size':50257,\n",
        "}\n",
        "\n",
        "x = torch.randint(10, (hparams['batch_size'], hparams['block_size'])) # batch size, sequence length\n",
        "\n",
        "model = GPT()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "losses_mha = []\n",
        "n_epochs = 10000\n",
        "for i in range(n_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    output, loss = model(x, x) # calculate loss internally\n",
        "\n",
        "#    loss = F.mse_loss(output, x) # Reconstruct input\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    losses_mha.append(loss.item())\n",
        "    if (i + 1) % 1000 == 0:\n",
        "        print(f\"Loss ({i+1}/{n_epochs}): {loss.item()}\")\n",
        "\n",
        "print(\" \")\n",
        "print(\"input_shape \", x.shape)\n",
        "print(\"output_shape \", output.shape)\n",
        "\n",
        "print(f\"\\nOutput:\\n{output[0,:3,:]}\\n\")\n",
        "print(f\"Query:\\n{x[0,:3]}\\n\")"
      ],
      "metadata": {
        "id": "vst9zL-8BO-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## compare with HF GPT2"
      ],
      "metadata": {
        "id": "GekJPmdzuiO1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtGUMaY5FiG4",
        "outputId": "37a2d2d4-007d-4162-8f7b-40351517d157"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialize a pretrained GPT model by copying over the weights\n",
        "#from a huggingface/transformers checkpoint.\n",
        "from transformers import GPT2LMHeadModel\n",
        "\n",
        "# init a huggingface/transformers model\n",
        "model_hf = GPT2LMHeadModel.from_pretrained('gpt2')\n",
        "print(\"HF GPT************** \\n\", model_hf)\n",
        "\n",
        "hparams = {\n",
        "    'n_layer':12, # num of blocks?\n",
        "    'batch_size':1,\n",
        "    'n_head': 12,\n",
        "    'n_embd':768, # 12 heads of size 64\n",
        "    'block_size':1024, # seq_len 3 words\n",
        "    'dropout':0.1, # dropout value\n",
        "    'bias': True, # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
        "    'vocab_size':50257,\n",
        "}\n",
        "\n",
        "# init our transformer model\n",
        "model = GPT()\n",
        "print(\"our GPT************** \\n\", model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vXmqjUOuGNXX",
        "outputId": "2c3aea6e-2132-42ea-d93d-f521a8346742"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "HF GPT************** \n",
            " GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D()\n",
            "          (c_proj): Conv1D()\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n",
            "number of parameters: 124.44M\n",
            "our GPT************** \n",
            " GPT(\n",
            "  (transformer): ModuleDict(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
            "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (act): GELU(approximate='none')\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sd_hf = model_hf.state_dict()\n",
        "sd = model.state_dict()\n",
        "\n",
        "sd_keys = sd.keys()\n",
        "sd_keys = [k for k in sd_keys if not k.endswith('.attn.tril')] # discard this mask / buffer, not a param\n",
        "\n",
        "\n",
        "\n",
        "# copy while ensuring all of the parameters are aligned and match in names and shapes\n",
        "sd_keys_hf = sd_hf.keys()\n",
        "sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
        "sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
        "transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
        "# basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
        "# this means that we have to transpose these weights when we import them\n",
        "assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
        "for k in sd_keys_hf:\n",
        "    if any(k.endswith(w) for w in transposed):\n",
        "        # special treatment for the Conv1D weights we need to transpose\n",
        "        assert sd_hf[k].shape[::-1] == sd[k].shape\n",
        "        with torch.no_grad():\n",
        "            sd[k].copy_(sd_hf[k].t())\n",
        "    else:\n",
        "        # vanilla copy over the other parameters\n",
        "        assert sd_hf[k].shape == sd[k].shape\n",
        "        with torch.no_grad():\n",
        "            sd[k].copy_(sd_hf[k])\n",
        "\n"
      ],
      "metadata": {
        "id": "6I8W9paWvA1K"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# compare the number of parameters\n",
        "# (note we don't count the decoder parameters in lm_head)\n",
        "print(\"Our-params:\", sum(p.numel() for p in model.parameters()))\n",
        "print(\"GPT-params:\", sum(p.numel() for p in model_hf.parameters()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fRTdMYMhM1W0",
        "outputId": "b3f9d827-1f41-4e47-b297-176eb136607e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Our-params: 163037184\n",
            "GPT-params: 124439808\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking for same outputs\n",
        "_ = model.eval()\n",
        "_ = model_hf.eval()\n",
        "\n",
        "idx = torch.tensor([[1,123,52,28]], dtype=torch.long)\n",
        "\n",
        "#logits_hf, _ = model_hf(idx)\n",
        "logits_hf = model_hf(idx).logits\n",
        "logits_our, _ = model(idx)\n",
        "\n",
        "print(\"hf__logits \", logits_hf)\n",
        "print(\"our_logits \", logits_our)\n",
        "print(\"\")\n",
        "print(\"hf__logits_shape \", logits_hf.shape)\n",
        "print(\"our_logits_shape \", logits_our.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nmVrdgagNo32",
        "outputId": "ab8aa16c-3a1b-4234-d414-41aacd408a34"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hf__logits  tensor([[[-32.9012, -31.2025, -34.6623,  ..., -39.4868, -39.8732, -32.2388],\n",
            "         [-74.0952, -71.2668, -74.4134,  ..., -84.2939, -82.6411, -75.3960],\n",
            "         [-75.3409, -74.9739, -77.7477,  ..., -86.1547, -83.4554, -78.8507],\n",
            "         [-78.8787, -80.1651, -80.8338,  ..., -89.4950, -89.4559, -83.1077]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "our_logits  tensor([[[-32.8416, -31.1477, -34.6075,  ..., -39.4235, -39.8091, -32.1826],\n",
            "         [-74.0748, -71.2478, -74.3932,  ..., -84.2734, -82.6181, -75.3790],\n",
            "         [-75.3062, -74.9425, -77.7129,  ..., -86.1231, -83.4241, -78.8159],\n",
            "         [-78.8222, -80.1078, -80.7757,  ..., -89.4309, -89.3999, -83.0506]]],\n",
            "       grad_fn=<UnsafeViewBackward0>)\n",
            "\n",
            "hf___logits_shape  torch.Size([1, 4, 50257])\n",
            "our__logits_shape  torch.Size([1, 4, 50257])\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}