{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Door Key Training with d3rlpy and DQN agent\n",
    "\n",
    "We will use the Door Key 16x16 environment from Minigrid Gym to train a DQN agent from d3rlpy which will act as an expert to collect data for our Decision Transformer agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if we are running on CoLab or not\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on CoLab')\n",
    "  !apt-get install -y xvfb ffmpeg > /dev/null 2>&1\n",
    "  %pip install pyvirtualdisplay pygame moviepy > /dev/null 2>&1\n",
    "  %pip install d3rlpy\n",
    "else:\n",
    "  print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory creation\n",
    "import os\n",
    "path = \"./models\"\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "  os.makedirs(path)\n",
    "\n",
    "path = \"./videos/video-doorkey-d3rlpy\"\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "  os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from minigrid.envs import DoorKeyEnv\n",
    "from gymnasium.core import ActType, ObsType\n",
    "from typing import Any, SupportsFloat\n",
    "import random, math\n",
    "\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id=\"MiniGrid-DoorKey-16x16-v0\",\n",
    "    entry_point=\"minigrid.envs:DoorKeyEnv\",\n",
    "    kwargs={\"size\": 16},\n",
    ")\n",
    "    \n",
    "class CurriculumStatesWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env, beta: int = 0.05):\n",
    "        \"\"\"A wrapper that adds an exploration bonus to less visited (state,action) pairs.\n",
    "\n",
    "        Args:\n",
    "            env: The environment to apply the wrapper\n",
    "        \"\"\"\n",
    "        super().__init__(env)\n",
    "        self.state_carrying = False\n",
    "        self.state_door_opened = False\n",
    "        self.count_frames = 0\n",
    "        self.door = None\n",
    "        self.key = None\n",
    "        self.total_reward = 0\n",
    "\n",
    "        self.beta = beta\n",
    "\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        \"\"\"Resets the environment.\"\"\"\n",
    "        \n",
    "        env : DoorKeyEnv = self.env.unwrapped\n",
    "       \n",
    "        #if (self.total_reward > 0.2):\n",
    "        #    print(\"Reward: \",self.total_reward)\n",
    "\n",
    "        obs = self.env.reset(**kwargs)\n",
    "\n",
    "        self.state_carrying = False\n",
    "        self.state_door_opened = False\n",
    "        self.count_frames = 0\n",
    "        self.count_carryings = 0\n",
    "        self.total_reward = 0\n",
    "        \n",
    "        # Randomize the position of the agent\n",
    "        curriculum = random.random()\n",
    "\n",
    "        for j in range(self.env.grid.height):\n",
    "            for i in range(self.env.grid.width):\n",
    "                tile = self.grid.get(i, j)\n",
    "                if tile != None and tile.type == \"door\":\n",
    "                    self.door = tile\n",
    "\n",
    "                if tile != None and tile.type == \"key\":\n",
    "                    self.key = tile\n",
    "\n",
    "\n",
    "        if curriculum < self.beta:\n",
    "            env.agent_pos = (-1, -1)\n",
    "            pos = env.place_obj(None, top=(0,0), size=None, max_tries=math.inf)\n",
    "            env.agent_pos = pos\n",
    "\n",
    "            self.door.state = 0\n",
    "            self.state_door_opened = True\n",
    "                        \n",
    "            env.carrying = self.key\n",
    "            self.grid.set(self.key.cur_pos[0], self.key.cur_pos[1], None)\n",
    "            self.carrying.cur_pos = np.array([-1, -1])\n",
    "            self.state_carrying = True\n",
    "\n",
    "\n",
    "\n",
    "        return obs\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Steps through the environment with `action`.\"\"\"\n",
    "\n",
    "        if (self.state_carrying == True and action == self.actions.drop):\n",
    "            action = self.actions.pickup     \n",
    "\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        env = self.unwrapped\n",
    "\n",
    "        self.count_frames = self.count_frames + 1\n",
    "\n",
    "        if (env.carrying is not None and self.state_carrying == False and self.count_carryings < 1):\n",
    "            self.count_carryings = self.count_carryings + 1\n",
    "            reward += 0.25\n",
    "            self.state_carrying = True\n",
    "\n",
    "        if self.door != None and self.door.is_open == True and self.actions.toggle and self.state_door_opened == False:\n",
    "            print(\"Is Opened\", self.door.is_open)\n",
    "            reward += 0.25\n",
    "            self.state_door_opened = True\n",
    "\n",
    "\n",
    "        self.total_reward += reward\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minigrid\n",
    "def create_env(env_key, max_episode_steps=100, is_video=False, curriculum_mode=True, beta=0.05):\n",
    "\n",
    "    render_mode = None\n",
    "\n",
    "    if is_video == True:\n",
    "        render_mode = 'rgb_array'\n",
    "\n",
    "    env = gym.make(env_key, max_episode_steps=max_episode_steps, render_mode=render_mode, see_through_walls=True)\n",
    "\n",
    "    if (curriculum_mode):\n",
    "        env = CurriculumStatesWrapper(env, beta=beta)\n",
    "        \n",
    "    env = minigrid.wrappers.FullyObsWrapper(env)\n",
    "    env = minigrid.wrappers.ImgObsWrapper(env)\n",
    "\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d3rlpy\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from d3rlpy.models.encoders import EncoderFactory\n",
    "\n",
    "\n",
    "env_key = \"MiniGrid-DoorKey-16x16-v0\"\n",
    "\n",
    "env = create_env(env_key, max_episode_steps=200, curriculum_mode=True)\n",
    "eval_env = create_env(env_key, max_episode_steps=200, curriculum_mode=True)\n",
    "\n",
    "class CustomConvEncoder(nn.Module):\n",
    "    def __init__(self, observation_shape):\n",
    "        super().__init__()\n",
    "        print(observation_shape)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=1, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2Dropout = nn.Dropout(0.25)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3Dropout = nn.Dropout(0.5)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4Dropout = nn.Dropout(0.5)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5Dropout = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x.permute(0, 3, 1, 2)\n",
    "        h = torch.relu(self.conv1(h))\n",
    "        h = torch.relu(self.conv2Dropout(self.conv2(h)))\n",
    "        h = torch.relu(self.conv3Dropout(self.conv3(h)))\n",
    "        h = torch.relu(self.conv4Dropout(self.conv4(h)))\n",
    "        h = torch.relu(self.conv5Dropout(self.conv5(h)))\n",
    "\n",
    "        h = torch.flatten(h, start_dim=1)\n",
    "        #print(h.shape)\n",
    "\n",
    "        return h\n",
    "    \n",
    "class CustomConvEncoderFactory(EncoderFactory):\n",
    "\n",
    "    def create(self, observation_shape):\n",
    "        return CustomConvEncoder(observation_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_type() -> str:\n",
    "        return \"custom\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn = d3rlpy.algos.DQNConfig(\n",
    "    encoder_factory=CustomConvEncoderFactory(),\n",
    "    batch_size=100,\n",
    "    gamma=0.9,\n",
    "    target_update_interval=1000,\n",
    "    learning_rate=2.5e-4\n",
    ").create(device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "from typing import Deque, List, Sequence, Tuple\n",
    "\n",
    "from typing_extensions import Protocol\n",
    "\n",
    "from d3rlpy.dataset.components import EpisodeBase\n",
    "\n",
    "from d3rlpy.dataset.buffers import BufferProtocol\n",
    "\n",
    "from d3rlpy.dataset.writers import ExperienceWriter, _ActiveEpisode, WriterPreprocessProtocol\n",
    "from d3rlpy.dataset.components import Signature\n",
    "\n",
    "\n",
    "class CustomReplayBuffer(d3rlpy.dataset.ReplayBuffer):\n",
    "\n",
    "    def clip_episode(self, terminated: bool) -> None:\n",
    "        r\"\"\"Clips the current episode.\n",
    "\n",
    "        Args:\n",
    "            terminated: Flag to represent environment termination.\n",
    "        \"\"\"\n",
    "\n",
    "        episode_to_remove = None\n",
    "        # Check if the episode's reward is 0 or negative\n",
    "        if not terminated and self._writer._active_episode.rewards.mean() <= 0:\n",
    "            episode_to_remove = self._writer._active_episode\n",
    "            \n",
    "        self._writer.clip_episode(terminated)\n",
    "\n",
    "        if episode_to_remove is not None:\n",
    "            # Remove all transitions associated with the episode to remove\n",
    "            self._buffer._transitions = [(ep, idx) for ep, idx in self._buffer._transitions if ep is not episode_to_remove]\n",
    "            self._buffer.episodes.remove(episode_to_remove)  \n",
    "\n",
    "\n",
    "class CustomWriterPreprocess(d3rlpy.dataset.WriterPreprocessProtocol):\n",
    "\n",
    "    def process_observation(self, observation: d3rlpy.types.Observation) -> d3rlpy.types.Observation:\n",
    "        return observation\n",
    "\n",
    "    def process_action(self, action: np.ndarray) -> np.ndarray:\n",
    "        #print(action)\n",
    "        return action\n",
    "\n",
    "    def process_reward(self, reward: np.ndarray) -> np.ndarray:\n",
    "        #if (reward >= 0.2):\n",
    "        #    print(reward)\n",
    "        return reward\n",
    "    \n",
    "writer_preprocessor = CustomWriterPreprocess()\n",
    "\n",
    "buffer = d3rlpy.dataset.FIFOBuffer(2000)\n",
    "buffer = CustomReplayBuffer(\n",
    "    buffer,\n",
    "    env=env, \n",
    "    writer_preprocessor=writer_preprocessor\n",
    ")\n",
    "\n",
    "explorer = d3rlpy.algos.LinearDecayEpsilonGreedy(0.8, 0.3)\n",
    "dqn.fit_online(\n",
    "    env,\n",
    "    buffer,\n",
    "    explorer,\n",
    "    n_steps=1000000,  # train for 100K steps\n",
    "    eval_env=eval_env,\n",
    "    n_steps_per_epoch=100000,  # evaluation is performed every 1K steps\n",
    "    update_start_step=20000,  # parameter update starts after 1K steps\n",
    "    update_interval=10\n",
    ")\n",
    "\n",
    "dqn.save_model(\"./models/model_door-key-dqn-16x16.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "env = create_env(env_key, max_episode_steps=100, curriculum_mode=True)\n",
    "buffer = d3rlpy.dataset.FIFOBuffer(2000)\n",
    "buffer = CustomReplayBuffer(\n",
    "    buffer,\n",
    "    env=env, \n",
    "    #observation_signature=observation_signature,\n",
    "    writer_preprocessor=writer_preprocessor\n",
    ")\n",
    "eval_env = create_env(env_key, max_episode_steps=150, curriculum_mode=True, beta=0)\n",
    "explorer = d3rlpy.algos.ConstantEpsilonGreedy(0.3)\n",
    "\n",
    "\n",
    "dqn.fit_online(\n",
    "    env,\n",
    "    buffer,\n",
    "    explorer,\n",
    "    n_steps=1000000,  # train for 100K steps\n",
    "    eval_env=eval_env,\n",
    "    n_steps_per_epoch=100000,  # evaluation is performed every 1K steps\n",
    "    update_start_step=20000,  # parameter update starts after 1K steps\n",
    "    update_interval=10\n",
    ")\n",
    "\n",
    "dqn.save_model(\"./models/model_door-key-dqn-16x16.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "# start virtual display\n",
    "d3rlpy.notebook_utils.start_virtual_display()\n",
    "\n",
    "env = create_env(env_key, max_episode_steps=200, is_video=True)\n",
    "\n",
    "env = RecordVideo(env, './videos/video-doorkey-d3rlpy')\n",
    "\n",
    "seed = 3\n",
    "\n",
    "explorer = d3rlpy.algos.ConstantEpsilonGreedy(0.3)\n",
    "done = False\n",
    "\n",
    "observation, reward = env.reset(seed=seed)\n",
    "\n",
    "while True:\n",
    "\n",
    "    x = np.expand_dims(observation, axis=0)\n",
    "    action = explorer.sample(dqn, x, 0)[0]\n",
    "\n",
    "    observation, reward, done, truncated, _ = env.step(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"reward:\", reward)\n",
    "        print(\"DONE!!!\")\n",
    "        break\n",
    "    elif truncated:\n",
    "        print(\"Truncated\")\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "d3rlpy.notebook_utils.render_video(\"./videos/video-doorkey-d3rlpy/rl-video-episode-0.mp4\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
