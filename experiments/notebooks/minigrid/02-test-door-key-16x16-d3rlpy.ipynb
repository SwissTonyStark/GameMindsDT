{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Door Key Offline Training with d3rlpy and Decision Transformer\n",
    "\n",
    "We will use the Door Key 16x16 environment from Minigrid Gym to test the Decision Transformer algorithm from d3rlpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if we are running on CoLab or not\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on CoLab')\n",
    "  !apt-get install -y xvfb ffmpeg > /dev/null 2>&1\n",
    "  %pip install pyvirtualdisplay pygame moviepy > /dev/null 2>&1\n",
    "  %pip install d3rlpy\n",
    "  %pip install matplotlib\n",
    "else:\n",
    "  print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory creation\n",
    "import os\n",
    "path = \"./models\"\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "  os.makedirs(path)\n",
    "\n",
    "path = \"./datasets\"\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "  os.makedirs(path)\n",
    "\n",
    "path = \"./videos/video-doorkey-dt-d3rlpy\"\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "  os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from minigrid.envs import DoorKeyEnv\n",
    "from gymnasium.core import ActType, ObsType\n",
    "from typing import Any, SupportsFloat\n",
    "import random, math\n",
    "import numpy as np\n",
    "\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id=\"MiniGrid-DoorKey-16x16-v0\",\n",
    "    entry_point=\"minigrid.envs:DoorKeyEnv\",\n",
    "    kwargs={\"size\": 16},\n",
    ")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a PartialObsWrapper to augment flattened partial observations with the agent's direction in a single vector. This is beneficial for training the Decision Transformer algorithm.\n",
    "\n",
    "Additionally, we developed a PartialAndFullyObsWrapper to enhance flattened partial observations with the agent's direction in just one vector, and to append the full observation to the partial observation. This is advantageous for sampling data from the environment using the trained DQN agent, and for saving the data in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minigrid\n",
    "from minigrid.core.constants import COLOR_TO_IDX, OBJECT_TO_IDX\n",
    "from gymnasium.core import ObservationWrapper\n",
    "\n",
    "class PartialObsWrapper(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        new_image_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(((self.env.agent_view_size * self.env.agent_view_size *  3) + 1),),  \n",
    "            dtype=\"uint8\",\n",
    "        )\n",
    "\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {**self.observation_space.spaces, \"image\": new_image_space}\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        image = np.concatenate((obs[\"image\"].flatten(), np.array([obs[\"direction\"]])))\n",
    "\n",
    "        return {\"image\": image}\n",
    "\n",
    "class PartialAndFullyObsWrapper(ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        new_image_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.env.width, self.env.height, 3),  # number of cells\n",
    "            dtype=\"uint8\",\n",
    "        )\n",
    "\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {**self.observation_space.spaces, \"image\": new_image_space}\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        env = self.unwrapped\n",
    "        full_grid = env.grid.encode()\n",
    "        full_grid[env.agent_pos[0]][env.agent_pos[1]] = np.array(\n",
    "            [OBJECT_TO_IDX[\"agent\"], COLOR_TO_IDX[\"red\"], env.agent_dir]\n",
    "        )\n",
    "\n",
    "        partial_image = np.concatenate((obs[\"image\"].flatten(), np.array([obs[\"direction\"]])))\n",
    "\n",
    "        return {\"partial_image\": partial_image, \"image\": full_grid}\n",
    "\n",
    "def create_env(env_key, max_episode_steps=100, isPartialObs=False, isFullyObs=True, is_video=False):\n",
    "    \n",
    "    render_mode = None\n",
    "\n",
    "    if is_video == True:\n",
    "        render_mode = 'rgb_array'\n",
    "\n",
    "    env = gym.make(env_key, max_episode_steps=max_episode_steps, render_mode=render_mode, see_through_walls=True)\n",
    "\n",
    "    if isPartialObs and isFullyObs:\n",
    "        env =  PartialAndFullyObsWrapper(env)\n",
    "    elif isPartialObs:\n",
    "        env = PartialObsWrapper(env)\n",
    "        env = minigrid.wrappers.ImgObsWrapper(env)\n",
    "    elif isFullyObs:\n",
    "        env = minigrid.wrappers.FullyObsWrapper(env)\n",
    "        env = minigrid.wrappers.ImgObsWrapper(env)\n",
    "\n",
    "    return env\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We retrieve the DQN agent trained in the notebook 'train-door-key-16x16-dqn.ipynb'. This agent is employed to sample data from the environment and to store this data in a dataset. This approach allows us to use the trained agent to generate expert data for the Decision Transformer algorithm. Additionally, this agent is utilized to assess the performance of the Decision Transformer algorithm. The agent is competent enough to solve the environment, though it is not flawless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d3rlpy\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from d3rlpy.models.encoders import EncoderFactory\n",
    "\n",
    "class CustomConvEncoder(nn.Module):\n",
    "    def __init__(self, observation_shape):\n",
    "        super().__init__()\n",
    "        print(observation_shape)\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=1, stride=1, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv2Dropout = nn.Dropout(0.25)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv3Dropout = nn.Dropout(0.5)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv4Dropout = nn.Dropout(0.5)\n",
    "        self.conv5 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n",
    "        self.conv5Dropout = nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = x.permute(0, 3, 1, 2)\n",
    "        h = torch.relu(self.conv1(h))\n",
    "        h = torch.relu(self.conv2Dropout(self.conv2(h)))\n",
    "        h = torch.relu(self.conv3Dropout(self.conv3(h)))\n",
    "        h = torch.relu(self.conv4Dropout(self.conv4(h)))\n",
    "        h = torch.relu(self.conv5Dropout(self.conv5(h)))\n",
    "\n",
    "        h = torch.flatten(h, start_dim=1)\n",
    "        #print(h.shape)\n",
    "\n",
    "        return h\n",
    "    \n",
    "class CustomConvEncoderFactory(EncoderFactory):\n",
    "\n",
    "    def create(self, observation_shape):\n",
    "        return CustomConvEncoder(observation_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_type() -> str:\n",
    "        return \"custom\"\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"seed\": 1,\n",
    "    \"dataset_size\": 10000,\n",
    "    \"epsilon\": 0.3,\n",
    "    \"max_episode_steps\": 200,\n",
    "    \"experiment_name\": \"door-key-16x16\",\n",
    "    \"device\": \"cuda:0\"\n",
    "}\n",
    "\n",
    "env_key = \"MiniGrid-DoorKey-16x16-v0\"\n",
    "\n",
    "env = create_env(env_key, max_episode_steps=config[\"max_episode_steps\"])\n",
    "\n",
    "dqn = d3rlpy.algos.DQNConfig(\n",
    "    encoder_factory=CustomConvEncoderFactory(),\n",
    "    batch_size=100,\n",
    "    gamma=0.9,\n",
    "    target_update_interval=1000,\n",
    "    learning_rate=2.5e-4\n",
    ").create(device=config[\"device\"])\n",
    "\n",
    "dqn.build_with_env(env)\n",
    "\n",
    "dqn.load_model('./models/model_door-key-dqn-16x16.pt')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We develop a CustomReplayBuffer to exclude episodes that have zero or negative rewards. Our goal is to retain only those episodes in which the agent is making progress towards the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CustomReplayBuffer(d3rlpy.dataset.ReplayBuffer):\n",
    "    ' Custom Replay Buffer to clip episodes with negative rewards'\n",
    "    \n",
    "    def clip_episode(self, terminated: bool) -> None:\n",
    "        r\"\"\"Clips the current episode.\n",
    "\n",
    "        Args:\n",
    "            terminated: Flag to represent environment termination.\n",
    "        \"\"\"\n",
    "\n",
    "        episode_to_remove = None\n",
    "        # Check if the episode's reward is 0 or negative\n",
    "        if not terminated and self._writer._active_episode.rewards.mean() <= 0:\n",
    "            episode_to_remove = self._writer._active_episode\n",
    "            \n",
    "        self._writer.clip_episode(terminated)\n",
    "\n",
    "        if episode_to_remove is not None:\n",
    "            # Remove all transitions associated with the episode to remove\n",
    "            self._buffer._transitions = [(ep, idx) for ep, idx in self._buffer._transitions if ep is not episode_to_remove]\n",
    "            self._buffer.episodes.remove(episode_to_remove)  \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We populate the buffer dataset with data sampled from the environment using the trained DQN agent. We utilize the PartialAndFullyObsWrapper to employ full observation for the DQN agent while saving the partial observation in the dataset. We continue sampling until the dataset is filled exclusively with positive rewards. To create a mix of non-expert and expert data, we initially introduce more entropy into the actions sampled from the DQN agent and gradually reduce the entropy to produce expert data. This is a slow process, but it is the only method we have found to generate expert data automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_data = create_env(env_key, max_episode_steps=config[\"max_episode_steps\"], isPartialObs=True, isFullyObs=False)\n",
    "env_sampler = create_env(env_key, max_episode_steps=config[\"max_episode_steps\"], isPartialObs=True, isFullyObs=True)\n",
    "\n",
    "buffer = d3rlpy.dataset.InfiniteBuffer()\n",
    "dataset = CustomReplayBuffer(buffer, env=env_data, cache_size=config[\"max_episode_steps\"])\n",
    "\n",
    "seed = 1\n",
    "\n",
    "done = False\n",
    "\n",
    "observation, _ = env_sampler.reset()\n",
    "\n",
    "num_steps = 0\n",
    "count_done = 0\n",
    "count_truncated = 0\n",
    "\n",
    "epsilon = 0.5\n",
    "epsilon_min = 0.1\n",
    "epsilon_decrement = (epsilon - epsilon_min) / 10  \n",
    "\n",
    "\n",
    "pbar = tqdm(total=config[\"dataset_size\"])\n",
    "\n",
    "while dataset.size() < config[\"dataset_size\"]:\n",
    "\n",
    "    pbar.update(dataset.size() - pbar.n) \n",
    "\n",
    "    if dataset.size() >= (config[\"dataset_size\"] * (1 - (epsilon - epsilon_min) / (epsilon - epsilon_decrement))):\n",
    "        epsilon = max(epsilon - epsilon_decrement, epsilon_min)\n",
    "\n",
    "    explorer = d3rlpy.algos.ConstantEpsilonGreedy(epsilon=epsilon)\n",
    "\n",
    "    while True:\n",
    "        x = np.expand_dims(observation['image'], axis=0)\n",
    "\n",
    "        action = explorer.sample(dqn, x, 0)[0]\n",
    "\n",
    "        next_observation, reward, done, truncated, _ = env_sampler.step(action)\n",
    "\n",
    "        clip_episode = done or truncated\n",
    "\n",
    "        # store observation\n",
    "        dataset.append(observation['partial_image'], action, float(reward))\n",
    "\n",
    "        # reset if terminated\n",
    "        if clip_episode:\n",
    "            dataset.clip_episode(done)\n",
    "            observation, _ = env_sampler.reset()\n",
    "            if done:\n",
    "                count_done += 1\n",
    "            if truncated:\n",
    "                count_truncated += 1\n",
    "            break\n",
    "        else:\n",
    "            observation = next_observation\n",
    "\n",
    "pbar.close()\n",
    "env.close()\n",
    "\n",
    "print(\"Dataset size: \", dataset.size())\n",
    "print(\"Truncated: \", count_truncated)\n",
    "\n",
    "dataset.dump('./datasets/dataset_door-key-dqn-16x16-mixed.d3')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the Discrete Decision Transformer agent using the data sampled from the environment. We employ the PartialObsWrapper to augment the partial observation with the agent's direction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_dt = create_env(env_key, max_episode_steps=config[\"max_episode_steps\"], isPartialObs=True, isFullyObs=False)\n",
    "env_dt_eval = create_env(env_key, max_episode_steps=config[\"max_episode_steps\"], isPartialObs=True, isFullyObs=False)\n",
    "\n",
    "buffer = d3rlpy.dataset.InfiniteBuffer()\n",
    "dataset = CustomReplayBuffer(buffer, env=env_dt, cache_size=config[\"max_episode_steps\"])\n",
    "\n",
    "dataset.load('./datasets/dataset_door-key-dqn-16x16-mixed.d3', buffer=buffer)\n",
    "\n",
    "\n",
    "target_return = 1\n",
    "experiment_name = config[\"experiment_name\"]\n",
    "experiment_seed = config[\"seed\"]\n",
    "\n",
    "d3rlpy.seed(experiment_seed)\n",
    "d3rlpy.envs.seed_env(env, experiment_seed)\n",
    "\n",
    "dt = d3rlpy.algos.DiscreteDecisionTransformerConfig(\n",
    "    batch_size=64,\n",
    "    encoder_factory=d3rlpy.models.VectorEncoderFactory(\n",
    "    ),\n",
    "    optim_factory=d3rlpy.models.GPTAdamWFactory(),\n",
    "    position_encoding_type=d3rlpy.PositionEncodingType.GLOBAL,\n",
    "    context_size=60,\n",
    "    num_heads=8,\n",
    "    num_layers=3,\n",
    "    max_timestep=200,\n",
    ").create(device=config[\"device\"])\n",
    "\n",
    "\n",
    "dt.fit(                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
    "    dataset,\n",
    "    n_steps=10000,\n",
    "    n_steps_per_epoch=2000,\n",
    "    eval_env=env_dt_eval,\n",
    "    eval_target_return=1,\n",
    "    eval_action_sampler=d3rlpy.algos.SoftmaxTransformerActionSampler(\n",
    "        temperature=1.0,\n",
    "    ),\n",
    "    experiment_name=f\"DT_{experiment_name}_{experiment_seed}\"\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make a video of the trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "\n",
    "# start virtual display\n",
    "d3rlpy.notebook_utils.start_virtual_display()\n",
    "\n",
    "env_video = create_env(env_key, max_episode_steps=200, is_video=True, isPartialObs=True, isFullyObs=False)\n",
    "\n",
    "env_video = RecordVideo(env_video, './videos/video-doorkey-dt-d3rlpy')\n",
    "\n",
    "# wrap as stateful actor for interaction\n",
    "actor = dt.as_stateful_wrapper(\n",
    "    target_return=1,\n",
    "    action_sampler=d3rlpy.algos.SoftmaxTransformerActionSampler(temperature=1.0,)\n",
    ")\n",
    "\n",
    "done = False\n",
    "\n",
    "actor.reset()\n",
    "observation, reward = env_video.reset()\n",
    "\n",
    "while True:\n",
    "\n",
    "    action = actor.predict(observation, 1) \n",
    "\n",
    "    observation, reward, done, truncated, _ = env_video.step(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"reward:\", reward)\n",
    "        print(\"DONE!!!\")\n",
    "        break\n",
    "    elif truncated:\n",
    "        print(\"Truncated\")\n",
    "        break\n",
    "\n",
    "env.close()\n",
    "\n",
    "d3rlpy.notebook_utils.render_video(\"./videos/video-doorkey-dt-d3rlpy/rl-video-episode-0.mp4\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt.save_model('./models/model_door-key-dt-16x16-128-d3rlpy.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = d3rlpy.load_learnable()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "We evaluate the DT-trained agent by comparing it with the DQN agent. We can see that the DT agent is superior to the DQN agent. The DT agent solves the environment more frequently than the DQN agent. The most important thing to observe is that the DT agent can solve the environment with only partial observation, and without being rewarded for picking up the key or opening the door. The DT agent manages to solve the environment solely with the reward for reaching the goal. Furthermore, the DT agent can solve the environment using only the data generated by the DQN agent. The DT agent is capable of learning from the data produced by the DQN agent. The result is an agent that is better than its 'teacher'. \n",
    "\n",
    "## Unestable Training\n",
    "Despite this, we have noticed that the training is very unstable, and it can yield very different results in each epoch. We need to continue investigating this issue a little bit more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to compare the performance of DQN and DT\n",
    "\n",
    "trials = 1000\n",
    "\n",
    "def evaluate_policy(env, actor, explorer=None, n_trials=10):\n",
    "\n",
    "    success = 0\n",
    "    for _ in tqdm(range(n_trials)):\n",
    "        \n",
    "        if (explorer is  None):\n",
    "            actor.reset()\n",
    "        obs,_ = env.reset()\n",
    "\n",
    "        done, truncated = False, False\n",
    "        while not (done or truncated):\n",
    "            if explorer is not None:\n",
    "                x = np.expand_dims(obs, axis=0)\n",
    "                action = explorer.sample(actor, x, 0)[0]\n",
    "            else:\n",
    "                action = actor.predict(obs, 1)\n",
    "            obs, reward, done, truncated, _ = env.step(action)\n",
    "            if done and reward > 0:\n",
    "                success += 1\n",
    "    return success / n_trials\n",
    "\n",
    "env_dqn = create_env(env_key, max_episode_steps=config[\"max_episode_steps\"], isPartialObs=False, isFullyObs=True)\n",
    "env_dt = create_env(env_key, max_episode_steps=config[\"max_episode_steps\"], isPartialObs=True, isFullyObs=False)\n",
    "\n",
    "dqn.build_with_env(env_dqn)\n",
    "dqn.load_model('./models/model_door-key-dqn-16x16.pt')\n",
    "\n",
    "#dt.build_with_env(env_dt)\n",
    "#dt.load_model('./models/model_door-key-dt-16x16-128-d3rlpy.pt')\n",
    "\n",
    "explorer_dqn = d3rlpy.algos.ConstantEpsilonGreedy(epsilon=config[\"epsilon\"])\n",
    "\n",
    "actor_dt = dt.as_stateful_wrapper(\n",
    "    target_return=1,\n",
    "    action_sampler=d3rlpy.algos.SoftmaxTransformerActionSampler(temperature=1.0,)\n",
    ")\n",
    "\n",
    "dqn_score = evaluate_policy(env_dqn, dqn, explorer=explorer_dqn, n_trials=trials)\n",
    "dt_score = evaluate_policy(env_dt, actor_dt, n_trials=trials)\n",
    "\n",
    "print(\"DQN score:\", dqn_score)\n",
    "print(\"DT score:\", dt_score)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a bar plot to compare the rewards of the DQN agent and the DT agent. It is apparent that the DT agent solves the environment more often than the DQN agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will make a bar chart to compare the performance of DQN and DT\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['DQN', 'DT']\n",
    "scores = [dqn_score, dt_score]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "rects1 = ax.bar(x - width/2, scores, width, label='Scores')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Scores by DQN and DT')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
