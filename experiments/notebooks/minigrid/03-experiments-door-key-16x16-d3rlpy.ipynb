{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Door Key Offline Training with d3rlpy and Decision Transformer. Comparing with BC and CQL\n",
    "\n",
    "We will make new experiments in Door Key 16x16 environment from Minigrid Gym with Offline algorithms from d3rlpy. \n",
    "\n",
    "We test the following offline algorithms:\n",
    "* Discrete Decision Transformer (DT)\n",
    "* Discrete Behaviour Clonig (BC)\n",
    "* Discrete CQL (CQL)\n",
    "\n",
    "## References\n",
    "We will use the docs of d3rlpy to describe the algorithms. \n",
    "https://d3rlpy.readthedocs.io/en/v2.3.0/references/algos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if we are running on CoLab or not\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on CoLab')\n",
    "  !apt-get install -y xvfb ffmpeg > /dev/null 2>&1\n",
    "  %pip install pyvirtualdisplay pygame moviepy > /dev/null 2>&1\n",
    "  %pip install d3rlpy\n",
    "  %pip install matplotlib\n",
    "else:\n",
    "  print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "path = \"./models\"\n",
    "path = \"./datasets\"\n",
    "path = \"./videos/video-doorkey-dt-d3rlpy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from minigrid.envs import DoorKeyEnv\n",
    "from gymnasium.core import ActType, ObsType\n",
    "from typing import Any, SupportsFloat\n",
    "import random, math\n",
    "import numpy as np\n",
    "\n",
    "from gymnasium.envs.registration import register\n",
    "\n",
    "register(\n",
    "    id=\"MiniGrid-DoorKey-16x16-v0\",\n",
    "    entry_point=\"minigrid.envs:DoorKeyEnv\",\n",
    "    kwargs={\"size\": 16},\n",
    ")\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We created a PartialObsWrapper to augment flattened partial observations with the agent's direction in a single vector. This is beneficial for training the Decision Transformer algorithm.\n",
    "\n",
    "Additionally, we developed a PartialAndFullyObsWrapper to enhance flattened partial observations with the agent's direction in just one vector, and to append the full observation to the partial observation. This is advantageous for sampling data from the environment using the trained DQN agent, and for saving the data in a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import minigrid\n",
    "from minigrid.core.constants import COLOR_TO_IDX, OBJECT_TO_IDX\n",
    "from gymnasium.core import ObservationWrapper\n",
    "\n",
    "class PartialObsWrapper(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        new_image_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(((self.env.agent_view_size * self.env.agent_view_size *  3) + 1),),  \n",
    "            dtype=\"uint8\",\n",
    "        )\n",
    "\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {**self.observation_space.spaces, \"image\": new_image_space}\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        image = np.concatenate((obs[\"image\"].flatten(), np.array([obs[\"direction\"]])))\n",
    "\n",
    "        return {\"image\": image}\n",
    "\n",
    "class PartialAndFullyObsWrapper(ObservationWrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "        new_image_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.env.width, self.env.height, 3),  # number of cells\n",
    "            dtype=\"uint8\",\n",
    "        )\n",
    "\n",
    "        self.observation_space = spaces.Dict(\n",
    "            {**self.observation_space.spaces, \"image\": new_image_space}\n",
    "        )\n",
    "\n",
    "    def observation(self, obs):\n",
    "        env = self.unwrapped\n",
    "        full_grid = env.grid.encode()\n",
    "        full_grid[env.agent_pos[0]][env.agent_pos[1]] = np.array(\n",
    "            [OBJECT_TO_IDX[\"agent\"], COLOR_TO_IDX[\"red\"], env.agent_dir]\n",
    "        )\n",
    "\n",
    "        partial_image = np.concatenate((obs[\"image\"].flatten(), np.array([obs[\"direction\"]])))\n",
    "\n",
    "        return {\"partial_image\": partial_image, \"image\": full_grid}\n",
    "\n",
    "def create_env(env_key, max_episode_steps=100, isPartialObs=False, isFullyObs=True, is_video=False):\n",
    "    \n",
    "    render_mode = None\n",
    "\n",
    "    if is_video == True:\n",
    "        render_mode = 'rgb_array'\n",
    "\n",
    "    env = gym.make(env_key, max_episode_steps=max_episode_steps, render_mode=render_mode, see_through_walls=True)\n",
    "\n",
    "    if isPartialObs and isFullyObs:\n",
    "        env =  PartialAndFullyObsWrapper(env)\n",
    "    elif isPartialObs:\n",
    "        env = PartialObsWrapper(env)\n",
    "        env = minigrid.wrappers.ImgObsWrapper(env)\n",
    "    elif isFullyObs:\n",
    "        env = minigrid.wrappers.FullyObsWrapper(env)\n",
    "        env = minigrid.wrappers.ImgObsWrapper(env)\n",
    "\n",
    "    return env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d3rlpy\n",
    "\n",
    "config = {\n",
    "    \"seed\": 1,\n",
    "    \"dataset_size\": 10000,\n",
    "    \"epsilon\": 0.3,\n",
    "    \"max_episode_steps\": 200,\n",
    "    \"experiment_name\": \"door-key-16x16\",\n",
    "    \"device\": \"cuda:0\"\n",
    "}\n",
    "\n",
    "env_key = \"MiniGrid-DoorKey-16x16-v0\"\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We develop a CustomReplayBuffer to exclude episodes that have zero or negative rewards. Our goal is to retain only those episodes in which the agent is making progress towards the goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class CustomReplayBuffer(d3rlpy.dataset.ReplayBuffer):\n",
    "    ' Custom Replay Buffer to clip episodes with negative rewards'\n",
    "    \n",
    "    def clip_episode(self, terminated: bool) -> None:\n",
    "        r\"\"\"Clips the current episode.\n",
    "\n",
    "        Args:\n",
    "            terminated: Flag to represent environment termination.\n",
    "        \"\"\"\n",
    "\n",
    "        episode_to_remove = None\n",
    "        # Check if the episode's reward is 0 or negative\n",
    "        if not terminated and self._writer._active_episode.rewards.mean() <= 0:\n",
    "            episode_to_remove = self._writer._active_episode\n",
    "            \n",
    "        self._writer.clip_episode(terminated)\n",
    "\n",
    "        if episode_to_remove is not None:\n",
    "            # Remove all transitions associated with the episode to remove\n",
    "            self._buffer._transitions = [(ep, idx) for ep, idx in self._buffer._transitions if ep is not episode_to_remove]\n",
    "            self._buffer.episodes.remove(episode_to_remove)  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = create_env(env_key, max_episode_steps=config[\"max_episode_steps\"], isPartialObs=True, isFullyObs=False)\n",
    "env_eval = create_env(env_key, max_episode_steps=config[\"max_episode_steps\"], isPartialObs=True, isFullyObs=False)\n",
    "\n",
    "buffer = d3rlpy.dataset.InfiniteBuffer()\n",
    "dataset = CustomReplayBuffer(buffer, env=env, cache_size=config[\"max_episode_steps\"])\n",
    "\n",
    "dataset.load('./datasets/dataset_door-key-dqn-16x16-mixed.d3', buffer=buffer)\n",
    "\n",
    "experiment_name = config[\"experiment_name\"]\n",
    "experiment_seed = config[\"seed\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Transformer\n",
    "Decision Transformer solves decision-making problems as a sequence modeling problem. It is a Transformer-based architecture that learns to predict the next action given the current state and the history of previous actions and states. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = d3rlpy.algos.DiscreteDecisionTransformerConfig(\n",
    "    batch_size=64,\n",
    "    encoder_factory=d3rlpy.models.VectorEncoderFactory(\n",
    "    ),\n",
    "    optim_factory=d3rlpy.models.GPTAdamWFactory(),\n",
    "    position_encoding_type=d3rlpy.PositionEncodingType.GLOBAL,\n",
    "    context_size=60,\n",
    "    num_heads=8,\n",
    "    num_layers=3,\n",
    "    max_timestep=200,\n",
    ").create(device=config[\"device\"])\n",
    "\n",
    "dt.fit(\n",
    "    dataset,\n",
    "    n_steps=5000,\n",
    "    n_steps_per_epoch=1000,\n",
    "    experiment_name=f\"DT_{experiment_name}_{experiment_seed}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Behaviour Cloning (BC)\n",
    "\n",
    "Behavior Cloning (BC) is to imitate actions in the dataset via a supervised learning approach. Since BC is only imitating action distributions, the performance will be close to the mean of the dataset even though BC mostly works better than online RL algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc = d3rlpy.algos.DiscreteBCConfig().create(device=\"cuda:0\")\n",
    "\n",
    "bc.fit(\n",
    "   dataset,\n",
    "   n_steps=10000,\n",
    "   n_steps_per_epoch=1000,\n",
    "   experiment_name=f\"BC_{experiment_name}_{experiment_seed}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conservative Q-Learning algorithm (CQL)\n",
    "\n",
    "Config of Conservative Q-Learning algorithm.\n",
    "\n",
    "CQL is a SAC-based data-driven deep reinforcement learning algorithm, which achieves state-of-the-art performance in offline RL problems.\n",
    "\n",
    "CQL mitigates overestimation error by minimizing action-values under the current policy and maximizing values under data distribution for underestimation issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cql = d3rlpy.algos.DiscreteCQLConfig().create(device=\"cuda:0\")\n",
    "\n",
    "cql.fit(\n",
    "   dataset,\n",
    "   n_steps=10000,\n",
    "   n_steps_per_epoch=1000,\n",
    "   experiment_name=f\"CQL_{experiment_name}_{experiment_seed}\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train the three algorithms with the same dataset, and we compare the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to compare the performance of DQN and DT\n",
    "\n",
    "trials = 1000\n",
    "\n",
    "def evaluate_policy(env, actor, explorer=None, n_trials=10):\n",
    "\n",
    "    success = 0\n",
    "    for _ in tqdm(range(n_trials)):\n",
    "        \n",
    "        if (explorer is  None):\n",
    "            actor.reset()\n",
    "        obs,_ = env.reset()\n",
    "\n",
    "        done, truncated = False, False\n",
    "        while not (done or truncated):\n",
    "            if explorer is not None:\n",
    "                x = np.expand_dims(obs, axis=0)\n",
    "                action = explorer.sample(actor, x, 0)[0]\n",
    "            else:\n",
    "                action = actor.predict(obs, 1)\n",
    "            obs, reward, done, truncated, _ = env.step(action)\n",
    "            if done and reward > 0:\n",
    "                success += 1\n",
    "    return success / n_trials\n",
    "\n",
    "env = create_env(env_key, max_episode_steps=config[\"max_episode_steps\"], isPartialObs=True, isFullyObs=False)\n",
    "\n",
    "actor_dt = dt.as_stateful_wrapper(\n",
    "    target_return=1,\n",
    "    action_sampler=d3rlpy.algos.SoftmaxTransformerActionSampler(temperature=1.0,)\n",
    ")\n",
    "dt_score = evaluate_policy(env, actor_dt, n_trials=trials)\n",
    "\n",
    "explorer = d3rlpy.algos.ConstantEpsilonGreedy(epsilon=config[\"epsilon\"])\n",
    "\n",
    "bc_score = evaluate_policy(env, bc, explorer, n_trials=trials)\n",
    "cql_score = evaluate_policy(env, cql, explorer, n_trials=trials)\n",
    "\n",
    "print(\"BC score:\", bc_score)\n",
    "print(\"CQL score:\", cql_score)\n",
    "print(\"DT score:\", dt_score)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will make a bar chart to compare the performance of DQN and DT\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['DT', 'BC', 'CQL']\n",
    "scores = [dt_score, bc_score, cql_score]\n",
    "\n",
    "x = np.arange(len(labels))  # the label locations\n",
    "width = 0.35  # the width of the bars\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "rects1 = ax.bar(x - width/2, scores, width, label='Scores')\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Scores')\n",
    "ax.set_title('Scores by offline algorithms DT, BC and CQL')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(labels)\n",
    "ax.legend()\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
