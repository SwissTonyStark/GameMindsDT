{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Door Key Offline Training with d3rlpy and Decision Transformer\n",
    "\n",
    "We will use the Door Key 16x16 environment from Minigrid Gym to test the Decision Transformer algorithm from d3rlpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if we are running on CoLab or not\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on CoLab')\n",
    "  !apt-get install -y xvfb ffmpeg > /dev/null 2>&1\n",
    "  %pip install pyvirtualdisplay pygame moviepy > /dev/null 2>&1\n",
    "  %pip install d3rlpy\n",
    "else:\n",
    "  print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory creation\n",
    "import os\n",
    "path = \"./models\"\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "  os.makedirs(path)\n",
    "\n",
    "path = \"./videos/video-doorkey-d3rlpy\"\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "  os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "from minigrid.envs import DoorKeyEnv\n",
    "from gymnasium.core import ActType, ObsType\n",
    "from typing import Any, SupportsFloat\n",
    "import random, math\n",
    "\n",
    "class CurriculumEnvWrapper(gym.Wrapper, gym.utils.RecordConstructorArgs):\n",
    "\n",
    "    def __init__(self, env: gym.Env, beta: int = 0.1):\n",
    "        gym.utils.RecordConstructorArgs.__init__(self)\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.beta = beta\n",
    "\n",
    "        # Observations are dictionaries containing an\n",
    "        # encoding of the grid and a textual 'mission' string\n",
    "        image_observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(self.unwrapped.grid.width, self.unwrapped.grid.height, 3),\n",
    "            dtype=\"uint8\",\n",
    "        )\n",
    "        self.env.unwrapped.observation_space[\"image\"] =  image_observation_space\n",
    "\n",
    "    def step(\n",
    "        self, action: ActType\n",
    "    ) -> tuple[ObsType, SupportsFloat, bool, bool, dict[str, Any]]:\n",
    "        obs:  gym.wrappers.LazyFrames\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        vis_mask = np.ones(shape=(self.grid.width, self.grid.height), dtype=bool)\n",
    "        image = self.grid.encode(vis_mask)\n",
    "        obs[\"image\"] = image\n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "\n",
    "        env : DoorKeyEnv = self.env.unwrapped\n",
    "\n",
    "        obs, dc = self.env.reset(**kwargs)\n",
    "\n",
    "        vis_mask = np.ones(shape=(self.unwrapped.grid.width, self.unwrapped.grid.height), dtype=bool)\n",
    "        image = self.unwrapped.grid.encode(vis_mask)\n",
    "\n",
    "        obs[\"image\"] = image\n",
    "        \n",
    "        # Randomize the position of the agent\n",
    "        curriculum = random.random()\n",
    "\n",
    "        if curriculum < self.beta:\n",
    "            env.agent_pos = (-1, -1)\n",
    "            pos = env.place_obj(None, top=(0,0), size=None, max_tries=math.inf)\n",
    "            env.agent_pos = pos\n",
    "\n",
    "        return obs, dc\n",
    "class NumpyStackWrapper(gym.Wrapper, gym.utils.RecordConstructorArgs):\n",
    "\n",
    "    def __init__(self, env: gym.Env, n: int = 4):\n",
    "        gym.utils.RecordConstructorArgs.__init__(self)\n",
    "        gym.Wrapper.__init__(self, env)\n",
    "        self.n = n\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "    \n",
    "        obs:  gym.wrappers.LazyFrames\n",
    "\n",
    "        obs, info = self.env.reset(**kwargs)\n",
    "\n",
    "        return obs.__array__(), info\n",
    "\n",
    "    def step(self, action):\n",
    "\n",
    "        obs:  gym.wrappers.LazyFrames\n",
    "\n",
    "        obs, reward, terminated, truncated, info  = self.env.step(action)\n",
    "\n",
    "        return obs.__array__(), reward, terminated, truncated, info \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d3rlpy\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from d3rlpy.models.encoders import EncoderFactory\n",
    "\n",
    "env_key = \"MiniGrid-DoorKey-16x16-v0\"\n",
    "\n",
    "def create_env(env_key, max_episode_steps=1000, is_video=False, curriculum_mode=False):\n",
    "\n",
    "    render_mode = None\n",
    "\n",
    "    if is_video == True:\n",
    "        render_mode = 'rgb_array'\n",
    "\n",
    "    env = gym.make(env_key, max_episode_steps=max_episode_steps, render_mode=render_mode, see_through_walls=True)\n",
    "\n",
    "    if (curriculum_mode):\n",
    "        env = CurriculumEnvWrapper(env)\n",
    "\n",
    "    if (is_video == False):\n",
    "        env = gym.wrappers.FilterObservation(env, filter_keys=['image','direction']) \n",
    "        env = gym.wrappers.FlattenObservation(env)\n",
    "        #env = gym.wrappers.FrameStack(env, 20)\n",
    "        #env = NumpyStackWrapper(env)\n",
    "        #env = gym.wrappers.FlattenObservation(env)\n",
    "\n",
    "    return env\n",
    "\n",
    "env = create_env(env_key, max_episode_steps=200, curriculum_mode=True)\n",
    "eval_env = create_env(env_key, max_episode_steps=200, curriculum_mode=True)\n",
    "\n",
    "class CustomEncoder(nn.Module):\n",
    "    def __init__(self, observation_shape):\n",
    "        super().__init__()\n",
    "        print(observation_shape)\n",
    "\n",
    "        self.fc1 = nn.Linear(observation_shape[0], 2048)\n",
    "        self.fc1dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(2048, 1024)\n",
    "        self.fc2dropout = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(1024, 512)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc2dropout(self.fc1(x)))\n",
    "        h = torch.relu(self.fc2dropout(self.fc2(h)))\n",
    "        h = torch.relu(self.fc3(h))\n",
    "        return h\n",
    "    \n",
    "class CustomEncoderFactory(EncoderFactory):\n",
    "\n",
    "    def create(self, observation_shape):\n",
    "        return CustomEncoder(observation_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_type() -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "dqn = d3rlpy.algos.DiscreteSACConfig(\n",
    "\n",
    ").create(device=\"cuda:0\")\n",
    "#dqn = d3rlpy.algos.DQNConfig(\n",
    "#    encoder_factory=CustomEncoderFactory(),\n",
    "#    batch_size=100,\n",
    "#    gamma=0.9,\n",
    "#    target_update_interval=1000,\n",
    "#    learning_rate=2.5e-4\n",
    "#).create(device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "from typing import Deque, List, Sequence, Tuple\n",
    "\n",
    "from typing_extensions import Protocol\n",
    "\n",
    "from d3rlpy.dataset.components import EpisodeBase\n",
    "\n",
    "from d3rlpy.dataset.buffers import BufferProtocol\n",
    "\n",
    "from d3rlpy.dataset.writers import ExperienceWriter, _ActiveEpisode, WriterPreprocessProtocol\n",
    "from d3rlpy.dataset.components import Signature\n",
    "\n",
    "\n",
    "class CustomReplayBuffer(d3rlpy.dataset.ReplayBuffer):\n",
    "\n",
    "    def clip_episode(self, terminated: bool) -> None:\n",
    "        r\"\"\"Clips the current episode.\n",
    "\n",
    "        Args:\n",
    "            terminated: Flag to represent environment termination.\n",
    "        \"\"\"\n",
    "\n",
    "        episode_to_remove = None\n",
    "        # Check if the episode's reward is 0 or negative\n",
    "        if not terminated and self._writer._active_episode.rewards.mean() <= 0:\n",
    "            episode_to_remove = self._writer._active_episode\n",
    "            \n",
    "        self._writer.clip_episode(terminated)\n",
    "\n",
    "        if episode_to_remove is not None:\n",
    "            # Remove all transitions associated with the episode to remove\n",
    "            self._buffer._transitions = [(ep, idx) for ep, idx in self._buffer._transitions if ep is not episode_to_remove]\n",
    "            self._buffer.episodes.remove(episode_to_remove)  \n",
    "\n",
    "\n",
    "class CustomWriterPreprocess(d3rlpy.dataset.WriterPreprocessProtocol):\n",
    "\n",
    "    def process_observation(self, observation: d3rlpy.types.Observation) -> d3rlpy.types.Observation:\n",
    "        return observation\n",
    "\n",
    "    def process_action(self, action: np.ndarray) -> np.ndarray:\n",
    "        #print(action)\n",
    "        return action\n",
    "\n",
    "    def process_reward(self, reward: np.ndarray) -> np.ndarray:\n",
    "        #if (reward > 0.1):\n",
    "        #    print(reward)\n",
    "        return reward\n",
    "    \n",
    "writer_preprocessor = CustomWriterPreprocess()\n",
    "\n",
    "#buffer = PriorityBuffer(200)\n",
    "buffer = d3rlpy.dataset.FIFOBuffer(20000)\n",
    "buffer = CustomReplayBuffer(\n",
    "    buffer,\n",
    "    env=env, \n",
    "    #observation_signature=observation_signature,\n",
    "    writer_preprocessor=writer_preprocessor\n",
    ")\n",
    "\n",
    "#buffer = d3rlpy.dataset.create_fifo_replay_buffer(\n",
    "#    limit=10000, env=env)\n",
    "\n",
    "explorer = d3rlpy.algos.LinearDecayEpsilonGreedy(0.9, 0.3)\n",
    "dqn.fit_online(\n",
    "    env,\n",
    "    buffer,\n",
    "    explorer,\n",
    "    n_steps=10000000,  # train for 100K steps\n",
    "    eval_env=eval_env,\n",
    "    n_steps_per_epoch=100000,  # evaluation is performed every 1K steps\n",
    "    update_start_step=10000,  # parameter update starts after 1K steps\n",
    "    update_interval=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "# start virtual display\n",
    "d3rlpy.notebook_utils.start_virtual_display()\n",
    "\n",
    "env = create_env(env_key, max_episode_steps=1000, curriculum_mode=True)\n",
    "# wrap RecordVideo wrapper\n",
    "env_video = create_env(env_key, max_episode_steps=1000, is_video=True, curriculum_mode=True)\n",
    "\n",
    "env_video = RecordVideo(env_video, './videos/video-doorkey-d3rlpy')\n",
    "\n",
    "seed = 2\n",
    "\n",
    "# interaction\n",
    "observation, reward = env.reset(seed=seed)\n",
    "\n",
    "env_video.reset(seed=seed)\n",
    "\n",
    "explorer = d3rlpy.algos.ConstantEpsilonGreedy(0.3)\n",
    "i = 0\n",
    "done = False\n",
    "\n",
    "while True:\n",
    "    #action = dqn.predict(np.expand_dims(observation, axis=0))[0]\n",
    "    x = np.expand_dims(observation, axis=0)\n",
    "\n",
    "    action = explorer.sample(dqn, x, 0)[0]\n",
    "\n",
    "    observation, reward, done, truncated, _ = env.step(action)\n",
    "    env_video.step(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"reward:\", reward)\n",
    "        print(\"DONE!!!\")\n",
    "        env_video.reset(seed=seed)\n",
    "        break\n",
    "    elif truncated:\n",
    "        print(\"Truncated\")\n",
    "        break\n",
    "\n",
    "\n",
    "d3rlpy.notebook_utils.render_video(\"./videos/video-doorkey-d3rlpy/rl-video-episode-0.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
