{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Door Key Offline Training with d3rlpy and Decision Transformer\n",
    "\n",
    "We will use the Door Key 16x16 environment from Minigrid Gym to test the Decision Transformer algorithm from d3rlpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test if we are running on CoLab or not\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "  print('Running on CoLab')\n",
    "  !apt-get install -y xvfb ffmpeg > /dev/null 2>&1\n",
    "  %pip install pyvirtualdisplay pygame moviepy > /dev/null 2>&1\n",
    "  %pip install d3rlpy\n",
    "else:\n",
    "  print('Not running on CoLab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory creation\n",
    "import os\n",
    "path = \"./models\"\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "  os.makedirs(path)\n",
    "\n",
    "path = \"./videos/video-doorkey-d3rlpy\"\n",
    "isExist = os.path.exists(path)\n",
    "if not isExist:\n",
    "  os.makedirs(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import d3rlpy\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from d3rlpy.models.encoders import EncoderFactory\n",
    "\n",
    "env_key = \"MiniGrid-DoorKey-8x8-v0\"\n",
    "\n",
    "def create_env(env_key, max_episode_steps=1000, render_mode=None):\n",
    "    env = gym.make(env_key, max_episode_steps=max_episode_steps, render_mode=render_mode)\n",
    "    print(max_episode_steps)\n",
    "    if (render_mode is None):\n",
    "        env = gym.wrappers.FilterObservation(env, filter_keys=['image','direction']) \n",
    "        env = gym.wrappers.FlattenObservation(env)\n",
    "        #env = gym.wrappers.NormalizeObservation(env)   \n",
    "    return env\n",
    "\n",
    "env = create_env(env_key, max_episode_steps=1000)\n",
    "eval_env = create_env(env_key, max_episode_steps=100)\n",
    "print(env.observation_space)\n",
    "\n",
    "class CustomEncoder(nn.Module):\n",
    "    def __init__(self, observation_shape):\n",
    "        super().__init__()\n",
    "        print(observation_shape)\n",
    "        self.feature_size = 16\n",
    "        self.fc1 = nn.Linear(observation_shape[0], 128)\n",
    "        self.fc1dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc2dropout = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc2dropout(self.fc1(x)))\n",
    "        h = torch.relu(self.fc2dropout(self.fc2(h)))\n",
    "        h = torch.relu(self.fc3(h))\n",
    "        return h\n",
    "    \n",
    "class CustomEncoderFactory(EncoderFactory):\n",
    "\n",
    "    def create(self, observation_shape):\n",
    "        return CustomEncoder(observation_shape)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_type() -> str:\n",
    "        return \"custom\"\n",
    "\n",
    "dqn = d3rlpy.algos.DQNConfig(\n",
    "    encoder_factory=CustomEncoderFactory(),\n",
    "    batch_size=100,\n",
    "    gamma=0.9,\n",
    "    target_update_interval=1000,\n",
    "    learning_rate=2.5e-4\n",
    ").create(device=\"cuda:0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "from typing import Deque, List, Sequence, Tuple\n",
    "\n",
    "from typing_extensions import Protocol\n",
    "\n",
    "from d3rlpy.dataset.components import EpisodeBase\n",
    "\n",
    "from d3rlpy.dataset.buffers import BufferProtocol\n",
    "\n",
    "from d3rlpy.dataset.writers import ExperienceWriter, _ActiveEpisode, WriterPreprocessProtocol\n",
    "from d3rlpy.dataset.components import Signature\n",
    "\n",
    "class PriorityBuffer:\n",
    "    r\"\"\"Priority buffer using a sorted list.\n",
    "\n",
    "    Args:\n",
    "        limit (int): buffer capacity.\n",
    "    \"\"\"\n",
    "    _transitions: List[Tuple[EpisodeBase, int]]\n",
    "    _episodes: List[EpisodeBase]\n",
    "    _limit: int\n",
    "\n",
    "    def __init__(self, limit: int):\n",
    "        self._limit = limit\n",
    "        self._transitions = []\n",
    "        self._episodes = []\n",
    "\n",
    "    def get_priority(self, episode: EpisodeBase) -> float:\n",
    "        return episode.rewards.mean()\n",
    "\n",
    "    def append(self, episode: EpisodeBase, index: int) -> None:\n",
    "        priority = self.get_priority(episode)\n",
    "        entry = (priority, (episode, index))\n",
    "        self._transitions.append(entry)\n",
    "        self._transitions.sort(key=lambda x: x[0])  # Sort by priority\n",
    "        if not self._episodes or episode is not self._episodes[-1]:\n",
    "            self._episodes.append(episode)\n",
    "        if len(self._transitions) > self._limit:\n",
    "            self._remove_lowest_priority()\n",
    "\n",
    "    def _remove_lowest_priority(self) -> None:\n",
    "        _, (episode, _) = self._transitions.pop(0)\n",
    "        if episode is self._episodes[0]:\n",
    "            self._episodes.pop(0)\n",
    "\n",
    "    @property\n",
    "    def episodes(self) -> Sequence[EpisodeBase]:\n",
    "        return self._episodes\n",
    "\n",
    "    @property\n",
    "    def transition_count(self) -> int:\n",
    "        return len(self._transitions)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self._transitions)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[EpisodeBase, int]:\n",
    "        _, (episode, idx) = self._transitions[index]\n",
    "        return episode, idx\n",
    "\n",
    "class CustomReplayBuffer(d3rlpy.dataset.ReplayBuffer):\n",
    "\n",
    "    def clip_episode(self, terminated: bool) -> None:\n",
    "        r\"\"\"Clips the current episode.\n",
    "\n",
    "        Args:\n",
    "            terminated: Flag to represent environment termination.\n",
    "        \"\"\"\n",
    "\n",
    "        episode_to_remove = None\n",
    "        # Check if the episode's reward is 0 or negative\n",
    "        if not terminated and self._writer._active_episode.rewards.mean() <= 0:\n",
    "            episode_to_remove = self._writer._active_episode\n",
    "            \n",
    "        self._writer.clip_episode(terminated)\n",
    "\n",
    "        if episode_to_remove is not None:\n",
    "            # Remove all transitions associated with the episode to remove\n",
    "            self._buffer._transitions = [(ep, idx) for ep, idx in self._buffer._transitions if ep is not episode_to_remove]\n",
    "            self._buffer.episodes.remove(episode_to_remove)  \n",
    "\n",
    "\n",
    "class CustomWriterPreprocess(d3rlpy.dataset.WriterPreprocessProtocol):\n",
    "\n",
    "    def process_observation(self, observation: d3rlpy.types.Observation) -> d3rlpy.types.Observation:\n",
    "        return observation\n",
    "\n",
    "    def process_action(self, action: np.ndarray) -> np.ndarray:\n",
    "        #print(action)\n",
    "        return action\n",
    "\n",
    "    def process_reward(self, reward: np.ndarray) -> np.ndarray:\n",
    "        if (reward > 0.1):\n",
    "            print(reward)\n",
    "        return reward\n",
    "    \n",
    "writer_preprocessor = CustomWriterPreprocess()\n",
    "\n",
    "#buffer = PriorityBuffer(200)\n",
    "buffer = d3rlpy.dataset.FIFOBuffer(10000)\n",
    "buffer = CustomReplayBuffer(\n",
    "    buffer,\n",
    "    env=env, \n",
    "    #observation_signature=observation_signature,\n",
    "    writer_preprocessor=writer_preprocessor\n",
    ")\n",
    "\n",
    "#buffer = d3rlpy.dataset.create_fifo_replay_buffer(\n",
    "#    limit=10000, env=env)\n",
    "\n",
    "explorer = d3rlpy.algos.LinearDecayEpsilonGreedy(0.9, 0.3)\n",
    "dqn.fit_online(\n",
    "    env,\n",
    "    buffer,\n",
    "    explorer,\n",
    "    n_steps=1000000,  # train for 100K steps\n",
    "    eval_env=eval_env,\n",
    "    n_steps_per_epoch=100000,  # evaluation is performed every 1K steps\n",
    "    update_start_step=50000,  # parameter update starts after 1K steps\n",
    "    update_interval=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "import numpy as np\n",
    "from gym.wrappers import RecordVideo\n",
    "\n",
    "# start virtual display\n",
    "d3rlpy.notebook_utils.start_virtual_display()\n",
    "\n",
    "env = create_env(env_key, max_episode_steps=1000)\n",
    "# wrap RecordVideo wrapper\n",
    "env_video = RecordVideo(gym.make(env_key, render_mode=\"rgb_array\"), './videos/video-doorkey-d3rlpy')\n",
    "\n",
    "seed = 1\n",
    "\n",
    "# interaction\n",
    "observation, reward = env.reset(seed=seed)\n",
    "env_video.reset(seed=seed)\n",
    "\n",
    "explorer = d3rlpy.algos.ConstantEpsilonGreedy(0.3)\n",
    "i = 0\n",
    "done = False\n",
    "\n",
    "while True:\n",
    "    #action = dqn.predict(np.expand_dims(observation, axis=0))[0]\n",
    "    x = np.expand_dims(observation, axis=0)\n",
    "    action = explorer.sample(dqn, x, 0)[0]\n",
    "\n",
    "    observation, reward, done, truncated, _ = env.step(action)\n",
    "    env_video.step(action)\n",
    "\n",
    "    if done:\n",
    "        print(\"reward:\", reward)\n",
    "        print(\"DONE!!!\")\n",
    "        env_video.reset(seed=seed)\n",
    "        break\n",
    "    elif truncated:\n",
    "        print(\"Truncated\")\n",
    "        break\n",
    "\n",
    "\n",
    "d3rlpy.notebook_utils.render_video(\"./videos/video-doorkey-d3rlpy/rl-video-episode-0.mp4\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "decision-transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
